<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jennifer HY Lin">
<meta name="dcterms.date" content="2025-01-08">

<title>Home - Building a simple deep learning model about adverse drug reactions</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/quarto-contrib/fontawesome6-1.2.0/all.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/fontawesome6-1.2.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-5CHVP3K63C"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-5CHVP3K63C', { 'anonymize_ip': true});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Home</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About me</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/jhylin"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://bsky.app/profile/jhylin.bsky.social">
 <span class="menu-text"><i class="fa-brands fa-bluesky" aria-label="bluesky"></i></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://fosstodon.org/@jhylin"><i class="bi bi-mastodon" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/jenhylin"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/jennifer-hy-lin-a9b86981/"><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://jhylin.github.io/CV_resume_quarto/">
 <span class="menu-text">CV</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div class="quarto-toggle-container">
                  <a href="" class="quarto-color-scheme-toggle nav-link" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Building a simple deep learning model about adverse drug reactions</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Deep learning</div>
                <div class="quarto-category">Pytorch</div>
                <div class="quarto-category">RDKit</div>
                <div class="quarto-category">Pandas</div>
                <div class="quarto-category">Python</div>
                <div class="quarto-category">ChEMBL database</div>
                <div class="quarto-category">Toxicology</div>
                <div class="quarto-category">Metabolism</div>
                <div class="quarto-category">Cheminformatics</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Jennifer HY Lin </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 8, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#import-libraries" id="toc-import-libraries" class="nav-link active" data-scroll-target="#import-libraries"><strong>Import libraries</strong></a></li>
  <li><a href="#import-adverse-drug-reactions-adrs-data" id="toc-import-adverse-drug-reactions-adrs-data" class="nav-link" data-scroll-target="#import-adverse-drug-reactions-adrs-data"><strong>Import adverse drug reactions (ADRs) data</strong></a></li>
  <li><a href="#import-smiles-data-from-chembl" id="toc-import-smiles-data-from-chembl" class="nav-link" data-scroll-target="#import-smiles-data-from-chembl"><strong>Import SMILES data from ChEMBL</strong></a></li>
  <li><a href="#merge-dataframes" id="toc-merge-dataframes" class="nav-link" data-scroll-target="#merge-dataframes"><strong>Merge dataframes</strong></a></li>
  <li><a href="#parse-smiles" id="toc-parse-smiles" class="nav-link" data-scroll-target="#parse-smiles"><strong>Parse SMILES</strong></a></li>
  <li><a href="#split-data" id="toc-split-data" class="nav-link" data-scroll-target="#split-data"><strong>Split data</strong></a></li>
  <li><a href="#locate-training-and-testing-sets-after-data-split" id="toc-locate-training-and-testing-sets-after-data-split" class="nav-link" data-scroll-target="#locate-training-and-testing-sets-after-data-split"><strong>Locate training and testing sets after data split</strong></a></li>
  <li><a href="#set-up-training-and-testing-sets-for-x-and-y-variables" id="toc-set-up-training-and-testing-sets-for-x-and-y-variables" class="nav-link" data-scroll-target="#set-up-training-and-testing-sets-for-x-and-y-variables"><strong>Set up training and testing sets for X and y variables</strong></a></li>
  <li><a href="#input-preprocessing-pipeline-using-pytorch-dataset-and-dataloader" id="toc-input-preprocessing-pipeline-using-pytorch-dataset-and-dataloader" class="nav-link" data-scroll-target="#input-preprocessing-pipeline-using-pytorch-dataset-and-dataloader"><strong>Input preprocessing pipeline using PyTorch Dataset and DataLoader</strong></a></li>
  <li><a href="#set-up-a-simple-dnn-regression-model" id="toc-set-up-a-simple-dnn-regression-model" class="nav-link" data-scroll-target="#set-up-a-simple-dnn-regression-model"><strong>Set up a simple DNN regression model</strong></a></li>
  <li><a href="#train-model" id="toc-train-model" class="nav-link" data-scroll-target="#train-model"><strong>Train model</strong></a></li>
  <li><a href="#evaluate-model" id="toc-evaluate-model" class="nav-link" data-scroll-target="#evaluate-model"><strong>Evaluate model</strong></a></li>
  <li><a href="#save-model" id="toc-save-model" class="nav-link" data-scroll-target="#save-model"><strong>Save model</strong></a></li>
  <li><a href="#reload-model" id="toc-reload-model" class="nav-link" data-scroll-target="#reload-model"><strong>Reload model</strong></a></li>
  <li><a href="#acknowledgements" id="toc-acknowledgements" class="nav-link" data-scroll-target="#acknowledgements"><strong>Acknowledgements</strong></a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/jhylin/Data_in_life_blog/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p><em>The notebook from <a href="https://github.com/jhylin/Adverse_drug_reactions">this repository</a> uses a venv created by <a href="https://docs.astral.sh/uv/pip/environments/#creating-a-virtual-environment">using uv</a> with <a href="https://docs.astral.sh/uv/guides/integration/jupyter/#using-jupyter-from-vs-code">a kernel set up this way</a>.</em></p>
<p><em>Some of the code blocks have been folded to keep the post length a bit more manageable - click on the code links to see full code (only applies to the HTML version, not the Jupyter notebook version).</em></p>
<p><br></p>
<section id="import-libraries" class="level5">
<h5 class="anchored" data-anchor-id="import-libraries"><strong>Import libraries</strong></h5>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn.functional <span class="im">import</span> one_hot</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> TensorDataset, DataLoader</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> datamol <span class="im">as</span> dm</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> rdkit</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> rdkit <span class="im">import</span> Chem</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> rdkit.Chem <span class="im">import</span> rdFingerprintGenerator</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> useful_rdkit_utils <span class="im">as</span> uru</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">'ignore'</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Pandas version used: </span><span class="sc">{</span>pd<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"PyTorch version used: </span><span class="sc">{</span>torch<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"NumPy version used: </span><span class="sc">{</span>np<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"RDKit version used: </span><span class="sc">{</span>rdkit<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Python version used: </span><span class="sc">{</span>sys<span class="sc">.</span>version<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Pandas version used: 2.2.3
PyTorch version used: 2.2.2
NumPy version used: 1.26.4
RDKit version used: 2024.09.4
Python version used: 3.12.7 (main, Oct 16 2024, 09:10:10) [Clang 18.1.8 ]</code></pre>
</div>
</div>
<p><br></p>
</section>
<section id="import-adverse-drug-reactions-adrs-data" class="level5">
<h5 class="anchored" data-anchor-id="import-adverse-drug-reactions-adrs-data"><strong>Import adverse drug reactions (ADRs) data</strong></h5>
<p>This is an extremely small set of data compiled manually (by me) via references stated in the dataframe. For details about what and how the data are collected, I’ve prepared a separate post as a <a href="https://jhylin.github.io/Data_in_life_blog/posts/22_Simple_dnn_adrs/1_ADR_data.html">data note</a> to explain key things about the data. It may not lead to a very significant result but it is done as an example of what an early or basic deep neural network (DNN) model may look like. Ideally there should be more training data and also more features added or used. I’ve hypothetically set the goal of this introductory piece to predict therapeutic drug classes from ADRs, molecular fingerprints and cytochrome P450 substrate strengths, but this won’t be achieved in this initial post (yet).</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.read_csv(<span class="st">"cyp3a4_substrates.csv"</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(data.shape)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>data.head(<span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(27, 8)</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="142">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>generic_drug_name</th>
      <th>notes</th>
      <th>cyp_strength_of_evidence</th>
      <th>drug_class</th>
      <th>adverse_drug_reactions</th>
      <th>first_ref</th>
      <th>second_ref</th>
      <th>date_checked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>carbamazepine</td>
      <td>NaN</td>
      <td>strong</td>
      <td>antiepileptics</td>
      <td>constipation^^, leucopenia^^, dizziness^^, som...</td>
      <td>drugs.com</td>
      <td>nzf</td>
      <td>211024</td>
    </tr>
    <tr>
      <th>1</th>
      <td>eliglustat</td>
      <td>NaN</td>
      <td>strong</td>
      <td>metabolic_agents</td>
      <td>diarrhea^^, oropharyngeal_pain^^, arthralgia^^...</td>
      <td>drugs.com</td>
      <td>emc</td>
      <td>151124</td>
    </tr>
    <tr>
      <th>2</th>
      <td>flibanserin</td>
      <td>NaN</td>
      <td>strong</td>
      <td>CNS_agents</td>
      <td>dizziness^^, somnolence^^, sedation^, fatigue^...</td>
      <td>drugs.com</td>
      <td>Drugs@FDA</td>
      <td>161124</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>For drug with astericks marked in “notes” column, see data notes under “Exceptions or notes for ADRs” section in <a href="https://jhylin.github.io/Data_in_life_blog/posts/22_Simple_dnn_adrs/1_ADR_data.html">separate post</a>.</p>
<p>I’m dropping some of the columns that are not going to be used later.</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> data.drop([</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"notes"</span>,</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"first_ref"</span>, </span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"second_ref"</span>, </span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"date_checked"</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    ], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>df.head(<span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="143">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>generic_drug_name</th>
      <th>cyp_strength_of_evidence</th>
      <th>drug_class</th>
      <th>adverse_drug_reactions</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>carbamazepine</td>
      <td>strong</td>
      <td>antiepileptics</td>
      <td>constipation^^, leucopenia^^, dizziness^^, som...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>eliglustat</td>
      <td>strong</td>
      <td>metabolic_agents</td>
      <td>diarrhea^^, oropharyngeal_pain^^, arthralgia^^...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>flibanserin</td>
      <td>strong</td>
      <td>CNS_agents</td>
      <td>dizziness^^, somnolence^^, sedation^, fatigue^...</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p><br></p>
</section>
<section id="import-smiles-data-from-chembl" class="level5">
<h5 class="anchored" data-anchor-id="import-smiles-data-from-chembl"><strong>Import SMILES data from ChEMBL</strong></h5>
<p>Before extracting data from ChEMBL, I’m getting a list of drug names in capital letters ready first which can be fed into chembl_downloader with my old cyp_drugs.py to retrieve the SMILES of these drugs.</p>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>string <span class="op">=</span> df[<span class="st">"generic_drug_name"</span>].tolist()</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert list of drugs into multiple strings of drug names</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>drugs <span class="op">=</span> <span class="ss">f"'</span><span class="sc">{</span><span class="st">"','"</span><span class="sc">.</span>join(string)<span class="sc">}</span><span class="ss">'"</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert from lower case to upper case</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> letter <span class="kw">in</span> drugs:</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> letter.islower():</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>        drugs <span class="op">=</span> drugs.replace(letter, letter.upper())</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(drugs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>'CARBAMAZEPINE','ELIGLUSTAT','FLIBANSERIN','IMATINIB','IBRUTINIB','NERATINIB','ESOMEPRAZOLE','OMEPRAZOLE','IVACAFTOR','NALOXEGOL','OXYCODONE','SIROLIMUS','TERFENADINE','DIAZEPAM','HYDROCORTISONE','LANSOPRAZOLE','PANTOPRAZOLE','LERCANIDIPINE','NALDEMEDINE','NELFINAVIR','TELAPREVIR','ONDANSETRON','QUININE','RIBOCICLIB','SUVOREXANT','TELITHROMYCIN','TEMSIROLIMUS'</code></pre>
</div>
</div>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get SMILES for each drug (via copying-and-pasting the previous cell output - attempted various ways to feed the string</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co"># directly into cyp_drugs.py, current way seems to be the most straightforward one...)</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> cyp_drugs <span class="im">import</span> chembl_drugs</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Using ChEMBL version 34</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>df_3a4 <span class="op">=</span> chembl_drugs(</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'CARBAMAZEPINE'</span>,<span class="st">'ELIGLUSTAT'</span>,<span class="st">'FLIBANSERIN'</span>,<span class="st">'IMATINIB'</span>,<span class="st">'IBRUTINIB'</span>,<span class="st">'NERATINIB'</span>,<span class="st">'ESOMEPRAZOLE'</span>,<span class="st">'OMEPRAZOLE'</span>,<span class="st">'IVACAFTOR'</span>,<span class="st">'NALOXEGOL'</span>,<span class="st">'OXYCODONE'</span>,<span class="st">'SIROLIMUS'</span>,<span class="st">'TERFENADINE'</span>,<span class="st">'DIAZEPAM'</span>,<span class="st">'HYDROCORTISONE'</span>,<span class="st">'LANSOPRAZOLE'</span>,<span class="st">'PANTOPRAZOLE'</span>,<span class="st">'LERCANIDIPINE'</span>,<span class="st">'NALDEMEDINE'</span>,<span class="st">'NELFINAVIR'</span>,<span class="st">'TELAPREVIR'</span>,<span class="st">'ONDANSETRON'</span>,<span class="st">'QUININE'</span>,<span class="st">'RIBOCICLIB'</span>,<span class="st">'SUVOREXANT'</span>,<span class="st">'TELITHROMYCIN'</span>,<span class="st">'TEMSIROLIMUS'</span>, </span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">#file_name="All_cyp3a4_smiles"</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df_3a4.shape)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>df_3a4.head(<span class="dv">3</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="co">## Note: latest ChEMBL version 35 (as from 1st Dec 2024) seems to be taking a long time to load (no output after ~7min), </span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="co">## both versions 33 &amp; 34 are ok with outputs loading within a few secs</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>(27, 4)</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="145">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>chembl_id</th>
      <th>pref_name</th>
      <th>max_phase</th>
      <th>canonical_smiles</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>CHEMBL108</td>
      <td>CARBAMAZEPINE</td>
      <td>4</td>
      <td>NC(=O)N1c2ccccc2C=Cc2ccccc21</td>
    </tr>
    <tr>
      <th>1</th>
      <td>CHEMBL12</td>
      <td>DIAZEPAM</td>
      <td>4</td>
      <td>CN1C(=O)CN=C(c2ccccc2)c2cc(Cl)ccc21</td>
    </tr>
    <tr>
      <th>2</th>
      <td>CHEMBL2110588</td>
      <td>ELIGLUSTAT</td>
      <td>4</td>
      <td>CCCCCCCC(=O)N[C@H](CN1CCCC1)[C@H](O)c1ccc2c(c1...</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p><br></p>
</section>
<section id="merge-dataframes" class="level5">
<h5 class="anchored" data-anchor-id="merge-dataframes"><strong>Merge dataframes</strong></h5>
<p>Next, I’m renaming the drug name column and merging the two dataframes together where one contains the ADRs and the other one contains the SMILES. I’m also making sure all drug names are in upper case for both dataframes so they can merge properly.</p>
<div class="cell" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Rename column &amp; change lower to uppercase</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.rename(columns<span class="op">=</span>{<span class="st">"generic_drug_name"</span>: <span class="st">"pref_name"</span>})</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>df[<span class="st">"pref_name"</span>] <span class="op">=</span> df[<span class="st">"pref_name"</span>].<span class="bu">str</span>.upper()</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Merge df &amp; df_3a4 </span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.merge(df_3a4, how<span class="op">=</span><span class="st">"left"</span>, on<span class="op">=</span><span class="st">"pref_name"</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>df.head(<span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="146">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>pref_name</th>
      <th>cyp_strength_of_evidence</th>
      <th>drug_class</th>
      <th>adverse_drug_reactions</th>
      <th>chembl_id</th>
      <th>max_phase</th>
      <th>canonical_smiles</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>CARBAMAZEPINE</td>
      <td>strong</td>
      <td>antiepileptics</td>
      <td>constipation^^, leucopenia^^, dizziness^^, som...</td>
      <td>CHEMBL108</td>
      <td>4</td>
      <td>NC(=O)N1c2ccccc2C=Cc2ccccc21</td>
    </tr>
    <tr>
      <th>1</th>
      <td>ELIGLUSTAT</td>
      <td>strong</td>
      <td>metabolic_agents</td>
      <td>diarrhea^^, oropharyngeal_pain^^, arthralgia^^...</td>
      <td>CHEMBL2110588</td>
      <td>4</td>
      <td>CCCCCCCC(=O)N[C@H](CN1CCCC1)[C@H](O)c1ccc2c(c1...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>FLIBANSERIN</td>
      <td>strong</td>
      <td>CNS_agents</td>
      <td>dizziness^^, somnolence^^, sedation^, fatigue^...</td>
      <td>CHEMBL231068</td>
      <td>4</td>
      <td>O=c1[nH]c2ccccc2n1CCN1CCN(c2cccc(C(F)(F)F)c2)CC1</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p><br></p>
</section>
<section id="parse-smiles" class="level5">
<h5 class="anchored" data-anchor-id="parse-smiles"><strong>Parse SMILES</strong></h5>
<p>Then I’m parsing the canonical SMILES through my old script to generate these small molecules as RDKit molecules and standardised SMILES, making sure these SMILES are parsable.</p>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Using my previous code to preprocess small mols</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="co"># disable rdkit messages</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>dm.disable_rdkit_log()</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co">#  The following function code were adapted from datamol.io</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> preprocess(row):</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Function to preprocess, fix, standardise, sanitise compounds </span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="co">    and then generate various molecular representations based on these molecules.</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Can be utilised as df.apply(preprocess, axis=1).</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="co">    :param smiles_column: SMILES column name (needs to be names as "canonical_smiles") </span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="co">    derived from ChEMBL database (or any other sources) via an input dataframe</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="co">    :param mol: RDKit molecules</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="co">    :return: preprocessed RDKit molecules, standardised SMILES, SELFIES, </span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a><span class="co">    InChI and InChI keys added as separate columns in the dataframe</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># smiles_column = strings object</span></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>    smiles_column <span class="op">=</span> <span class="st">"canonical_smiles"</span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert each compound into a RDKit molecule in the smiles column</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>    mol <span class="op">=</span> dm.to_mol(row[smiles_column], ordered<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Fix common errors in the molecules</span></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>    mol <span class="op">=</span> dm.fix_mol(mol)</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sanitise the molecules </span></span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>    mol <span class="op">=</span> dm.sanitize_mol(mol, sanifix<span class="op">=</span><span class="va">True</span>, charge_neutral<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Standardise the molecules</span></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>    mol <span class="op">=</span> dm.standardize_mol(</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>        mol,</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Switch on to disconnect metal ions</span></span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>        disconnect_metals<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>        normalize<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>        reionize<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Switch on "uncharge" to neutralise charges</span></span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>        uncharge<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Taking care of stereochemistries of compounds</span></span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Note: this uses the older approach of "AssignStereochemistry()" from RDKit</span></span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># https://github.com/datamol-io/datamol/blob/main/datamol/mol.py#L488</span></span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a>        stereo<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Adding following rows of different molecular representations </span></span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a>    row[<span class="st">"rdkit_mol"</span>] <span class="op">=</span> dm.to_mol(mol)</span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a>    row[<span class="st">"standard_smiles"</span>] <span class="op">=</span> dm.standardize_smiles(<span class="bu">str</span>(dm.to_smiles(mol)))</span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a>    <span class="co">#row["selfies"] = dm.to_selfies(mol)</span></span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a>    <span class="co">#row["inchi"] = dm.to_inchi(mol)</span></span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a>    <span class="co">#row["inchikey"] = dm.to_inchikey(mol)</span></span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> row</span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a>df_p3a4 <span class="op">=</span> df.<span class="bu">apply</span>(preprocess, axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df_p3a4.shape)</span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a>df_p3a4.head(<span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>(27, 9)</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="147">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>pref_name</th>
      <th>cyp_strength_of_evidence</th>
      <th>drug_class</th>
      <th>adverse_drug_reactions</th>
      <th>chembl_id</th>
      <th>max_phase</th>
      <th>canonical_smiles</th>
      <th>rdkit_mol</th>
      <th>standard_smiles</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>CARBAMAZEPINE</td>
      <td>strong</td>
      <td>antiepileptics</td>
      <td>constipation^^, leucopenia^^, dizziness^^, som...</td>
      <td>CHEMBL108</td>
      <td>4</td>
      <td>NC(=O)N1c2ccccc2C=Cc2ccccc21</td>
      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x1472ca030&gt;</td>
      <td>NC(=O)N1c2ccccc2C=Cc2ccccc21</td>
    </tr>
    <tr>
      <th>1</th>
      <td>ELIGLUSTAT</td>
      <td>strong</td>
      <td>metabolic_agents</td>
      <td>diarrhea^^, oropharyngeal_pain^^, arthralgia^^...</td>
      <td>CHEMBL2110588</td>
      <td>4</td>
      <td>CCCCCCCC(=O)N[C@H](CN1CCCC1)[C@H](O)c1ccc2c(c1...</td>
      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x1472c8040&gt;</td>
      <td>CCCCCCCC(=O)N[C@H](CN1CCCC1)[C@H](O)c1ccc2c(c1...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>FLIBANSERIN</td>
      <td>strong</td>
      <td>CNS_agents</td>
      <td>dizziness^^, somnolence^^, sedation^, fatigue^...</td>
      <td>CHEMBL231068</td>
      <td>4</td>
      <td>O=c1[nH]c2ccccc2n1CCN1CCN(c2cccc(C(F)(F)F)c2)CC1</td>
      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x1472cbca0&gt;</td>
      <td>O=c1[nH]c2ccccc2n1CCN1CCN(c2cccc(C(F)(F)F)c2)CC1</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p><br></p>
</section>
<section id="split-data" class="level5">
<h5 class="anchored" data-anchor-id="split-data"><strong>Split data</strong></h5>
<p>Random splits usually lead to overly optimistic models, where testing molecules are too similar to traininig molecules leading to many problems. This is further discussed in two other blog posts that I’ve found useful - <a href="https://greglandrum.github.io/rdkit-blog/posts/2024-05-31-scaffold-splits-and-murcko-scaffolds1.html">post by Greg Landrum</a> and <a href="https://practicalcheminformatics.blogspot.com/2024/11/some-thoughts-on-splitting-chemical.html">post by Pat Walters</a>.</p>
<p>Here I’m trying out Pat’s <a href="https://github.com/PatWalters/useful_rdkit_utils">useful_rdkit_utils</a>’ GroupKFoldShuffle code (code originated from <a href="https://github.com/scikit-learn/scikit-learn/issues/20520">this thread</a>) to split data (Butina clustering/splits). To do this, it requires SMILES to generate molecular fingerprints which will be used in the training and testing sets (potentially for future posts and in real-life cases, more things can be done with the SMILES or other molecular representations for machine learning, but to keep this post easy-to-read, I’ll stick with only generating the Morgan fingerprints for now).</p>
<div class="cell" data-execution_count="8">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate numpy arrays containing the fingerprints </span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>df_p3a4[<span class="st">'fp'</span>] <span class="op">=</span> df_p3a4.rdkit_mol.<span class="bu">apply</span>(rdFingerprintGenerator.GetMorganGenerator().GetCountFingerprintAsNumPy)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Get Butina cluster labels</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>df_p3a4[<span class="st">"butina_cluster"</span>] <span class="op">=</span> uru.get_butina_clusters(df_p3a4.standard_smiles)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up a GroupKFoldShuffle object</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>group_kfold_shuffle <span class="op">=</span> uru.GroupKFoldShuffle(n_splits<span class="op">=</span><span class="dv">5</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Using cross-validation/doing data split</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="co">## X = np.stack(df_s3a4.fp), y = df.adverse_drug_reactions, group labels = df_s3a4.butina_cluster</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train, test <span class="kw">in</span> group_kfold_shuffle.split(np.stack(df_p3a4.fp), df.adverse_drug_reactions, df_p3a4.butina_cluster):</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="bu">len</span>(train),<span class="bu">len</span>(test))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>20 7
23 4
23 4
23 4
19 8</code></pre>
</div>
</div>
<p><br></p>
</section>
<section id="locate-training-and-testing-sets-after-data-split" class="level5">
<h5 class="anchored" data-anchor-id="locate-training-and-testing-sets-after-data-split"><strong>Locate training and testing sets after data split</strong></h5>
<p>While trying to figure out how to locate training and testing sets after the data split, I’ve gone into a mini rabbit hole myself (a self-confusing session but gladly it clears up when my thought process goes further…). For example, some of the ways I’ve planned to try: create a dictionary as {index: butina label} first - butina cluster labels vs.&nbsp;index e.g.&nbsp;df_s3a4[“butina_cluster”], or maybe can directly convert from NumPy array to tensor - will need to locate drugs via indices first to specify training and testing sets, e.g.&nbsp;torch_train = torch.from_numpy(train) or torch_test = torch.from_numpy(test). It is actually simpler than this, which is to use pd.DataFrame.iloc() as shown below.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Training set indices</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>train</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="149">
<pre><code>array([ 0,  1,  3,  4,  5,  8,  9, 10, 12, 13, 14, 17, 18, 19, 20, 21, 23,
       24, 25])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># What df_p3a4 now looks like after data split - with "fp" and "butina_cluster" columns added</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>df_p3a4.head(<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="150">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>pref_name</th>
      <th>cyp_strength_of_evidence</th>
      <th>drug_class</th>
      <th>adverse_drug_reactions</th>
      <th>chembl_id</th>
      <th>max_phase</th>
      <th>canonical_smiles</th>
      <th>rdkit_mol</th>
      <th>standard_smiles</th>
      <th>fp</th>
      <th>butina_cluster</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>CARBAMAZEPINE</td>
      <td>strong</td>
      <td>antiepileptics</td>
      <td>constipation^^, leucopenia^^, dizziness^^, som...</td>
      <td>CHEMBL108</td>
      <td>4</td>
      <td>NC(=O)N1c2ccccc2C=Cc2ccccc21</td>
      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x1472ca030&gt;</td>
      <td>NC(=O)N1c2ccccc2C=Cc2ccccc21</td>
      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>
      <td>20</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert indices into list</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>train_set <span class="op">=</span> train.tolist()</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Locate drugs and drug info via pd.DataFrame.iloc</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>df_train <span class="op">=</span> df_p3a4.iloc[train_set]</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df_train.shape)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>df_train.head(<span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(19, 11)</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="151">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>pref_name</th>
      <th>cyp_strength_of_evidence</th>
      <th>drug_class</th>
      <th>adverse_drug_reactions</th>
      <th>chembl_id</th>
      <th>max_phase</th>
      <th>canonical_smiles</th>
      <th>rdkit_mol</th>
      <th>standard_smiles</th>
      <th>fp</th>
      <th>butina_cluster</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>CARBAMAZEPINE</td>
      <td>strong</td>
      <td>antiepileptics</td>
      <td>constipation^^, leucopenia^^, dizziness^^, som...</td>
      <td>CHEMBL108</td>
      <td>4</td>
      <td>NC(=O)N1c2ccccc2C=Cc2ccccc21</td>
      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x1472ca030&gt;</td>
      <td>NC(=O)N1c2ccccc2C=Cc2ccccc21</td>
      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>
      <td>20</td>
    </tr>
    <tr>
      <th>1</th>
      <td>ELIGLUSTAT</td>
      <td>strong</td>
      <td>metabolic_agents</td>
      <td>diarrhea^^, oropharyngeal_pain^^, arthralgia^^...</td>
      <td>CHEMBL2110588</td>
      <td>4</td>
      <td>CCCCCCCC(=O)N[C@H](CN1CCCC1)[C@H](O)c1ccc2c(c1...</td>
      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x1472c8040&gt;</td>
      <td>CCCCCCCC(=O)N[C@H](CN1CCCC1)[C@H](O)c1ccc2c(c1...</td>
      <td>[0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>
      <td>19</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Testing set indices</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>test</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="152">
<pre><code>array([ 2,  6,  7, 11, 15, 16, 22, 26])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>test_set <span class="op">=</span> test.tolist()</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>df_test <span class="op">=</span> df_p3a4.iloc[test_set]</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df_test.shape)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>df_test.head(<span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(8, 11)</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="153">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>pref_name</th>
      <th>cyp_strength_of_evidence</th>
      <th>drug_class</th>
      <th>adverse_drug_reactions</th>
      <th>chembl_id</th>
      <th>max_phase</th>
      <th>canonical_smiles</th>
      <th>rdkit_mol</th>
      <th>standard_smiles</th>
      <th>fp</th>
      <th>butina_cluster</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2</th>
      <td>FLIBANSERIN</td>
      <td>strong</td>
      <td>CNS_agents</td>
      <td>dizziness^^, somnolence^^, sedation^, fatigue^...</td>
      <td>CHEMBL231068</td>
      <td>4</td>
      <td>O=c1[nH]c2ccccc2n1CCN1CCN(c2cccc(C(F)(F)F)c2)CC1</td>
      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x1472cbca0&gt;</td>
      <td>O=c1[nH]c2ccccc2n1CCN1CCN(c2cccc(C(F)(F)F)c2)CC1</td>
      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>
      <td>18</td>
    </tr>
    <tr>
      <th>6</th>
      <td>ESOMEPRAZOLE</td>
      <td>strong</td>
      <td>proton_pump_inhibitors</td>
      <td>headache^^, flatulence^^, dizziness^, somnolen...</td>
      <td>CHEMBL1201320</td>
      <td>4</td>
      <td>COc1ccc2[nH]c([S@@+]([O-])Cc3ncc(C)c(OC)c3C)nc2c1</td>
      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x14733bd80&gt;</td>
      <td>COc1ccc2[nH]c([S@@+]([O-])Cc3ncc(C)c(OC)c3C)nc2c1</td>
      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p><br></p>
</section>
<section id="set-up-training-and-testing-sets-for-x-and-y-variables" class="level5">
<h5 class="anchored" data-anchor-id="set-up-training-and-testing-sets-for-x-and-y-variables"><strong>Set up training and testing sets for X and y variables</strong></h5>
<p>This part involves converting X (features) and y (target) variables into either one-hot encodings or vector embeddings, since I’ll be dealing with categories/words/ADRs and not numbers, and also to split each X and y variables into training and testing sets. At the very beginning, I’ve thought about using scikit_learn’s train_test_split(), but then realised that I should not need to do this as it’s already been done in the previous step (obviously I’m confusing myself again…). Essentially, this step can be integrated with the one-hot encoding and vector embeddings part as shown below.</p>
<p>There are three coding issues that have triggered warning messages when I’m trying to figure out how to convert CYP strengths into one-hot encodings:</p>
<ol type="1">
<li><p>A useful <a href="https://github.com/pandas-dev/pandas/issues/57734">thread</a> has helped me to solve the downcasting issue in pd.DataFrame.replace() when trying to do one-hot encoding to replace the CYP strengths for each drug</p></li>
<li><p>A Pandas setting-with-copy warning shows if using df[“column_name”]:</p></li>
</ol>
<blockquote class="blockquote">
<p>A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead</p>
</blockquote>
<p>The solution is to enable the copy-on-write globally (as commented in the code below; from <a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/copy_on_write.html#copy-on-write-cow">Pandas reference</a>).</p>
<ol start="3" type="1">
<li>PyTorch user warning appers if using df_train[“cyp_strength_of_evidence”].values, as this leads to non-writable tensors with a warning like this:</li>
</ol>
<blockquote class="blockquote">
<p>UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:212.)</p>
</blockquote>
<p>One of the solutions is to add copy() e.g.&nbsp;col_encoded = one_hot(torch.from_numpy(df[“column_name”].values.copy()) % total_numbers_in_column) or alternatively, convert column into numpy array first, then make the numpy array writeable (which is what I’ve used in the code below).</p>
<div class="cell" data-execution_count="14">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co">## X_train</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Convert "cyp_strength_of_evidence" column into one-hot encoding</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Enable copy-on-write globally to remove the warning</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>pd.options.mode.copy_on_write <span class="op">=</span> <span class="va">True</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Replace CYP strength as numbers</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> pd.option_context(<span class="st">'future.no_silent_downcasting'</span>, <span class="va">True</span>):</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>   df_train[<span class="st">"cyp_strength_of_evidence"</span>] <span class="op">=</span> df_train[<span class="st">"cyp_strength_of_evidence"</span>].replace({<span class="st">"strong"</span>: <span class="dv">1</span>, <span class="st">"mod"</span>: <span class="dv">2</span>}).infer_objects()</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>   df_test[<span class="st">"cyp_strength_of_evidence"</span>] <span class="op">=</span> df_test[<span class="st">"cyp_strength_of_evidence"</span>].replace({<span class="st">"strong"</span>: <span class="dv">1</span>, <span class="st">"mod"</span>: <span class="dv">2</span>}).infer_objects()</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Get total number of CYP strengths in df</span></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>total_cyp_str_train <span class="op">=</span> <span class="bu">len</span>(<span class="bu">set</span>(df_train[<span class="st">"cyp_strength_of_evidence"</span>]))</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert column into numpy array first, then make the numpy array writeable</span></span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>cyp_array_train <span class="op">=</span> df_train[<span class="st">"cyp_strength_of_evidence"</span>].to_numpy()</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>cyp_array_train.flags.writeable <span class="op">=</span> <span class="va">True</span></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>cyp_str_train_t <span class="op">=</span> one_hot(torch.from_numpy(cyp_array_train) <span class="op">%</span> total_cyp_str_train)</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>cyp_str_train_t</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="154">
<pre><code>tensor([[0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [0, 1],
        [1, 0],
        [1, 0],
        [1, 0],
        [1, 0],
        [1, 0],
        [1, 0],
        [1, 0],
        [1, 0],
        [1, 0],
        [1, 0]])</code></pre>
</div>
</div>
<p>Without going into too much details about vector embeddings (as there are a lot of useful learning materials about it online and in texts), here’s roughly how I understand embeddings while working on this post. Embeddings are real-valued dense vectors that are normally in multi-dimensional arrays and they can represent and catch the context of a word or sentence, the semantic similarity and especially the relation of each word with other words in a corpus of texts. They roughly form the basis of natural language processing and also contribute to how large language models are built… in a very simplified sense, but obviously this can get complex if we want the models to do more. Here, I’m trying something experimental so I’m going to convert each ADR for each drug into embeddings.</p>
<div class="cell" data-execution_count="15">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Convert "adverse_drug_reactions" column into embeddings</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="co">## see separate scripts used previously e.g. words_tensors.py </span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="co">## or Tensors_for_adrs_interactive.py to show step-by-step conversions from words to tensors</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Save all ADRs from common ADRs column as a list (joining every row of ADRs in place only)</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>adr_str_train <span class="op">=</span> df_train[<span class="st">"adverse_drug_reactions"</span>].tolist()</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Join separate rows of strings into one complete string</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>adr_string_train <span class="op">=</span> <span class="st">","</span>.join(adr_str_train)</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Converting all ADRs into Torch tensors using words_tensors.py</span></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> words_tensors <span class="im">import</span> words_tensors</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>adr_train_t <span class="op">=</span> words_tensors(adr_string_train)</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>adr_train_t</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="155">
<pre><code>tensor([[ 0.2880, -1.1804],
        [ 0.6725,  0.9416],
        [-1.7143,  0.3996],
        ...,
        [ 0.3675, -0.1449],
        [ 0.3549, -1.1478],
        [-1.4670, -0.6858]], grad_fn=&lt;EmbeddingBackward0&gt;)</code></pre>
</div>
</div>
<p>When trying to convert the “fp” column into tensors, there is one coding issue I’ve found relating to the data split step earlier. Each time the notebook is re-run with the kernel refreshed, the data split will lead to different proportions of training and testing sets due to the “shuffle = True”, which subsequently leads to different training and testing set arrays. One of the ways to circumvent this is to turn off the shuffle but this is not ideal for model training. So an alternative way that I’ve tried is to use ndarray.size (which is the product of elements in ndarray.shape, equivalent to multiplying the numbers of rows and columns), and divide the row of the intended tensor shape by 2 as I’m trying to reshape training arrays so they’re all in 2 columns in order for <a href="https://pytorch.org/docs/stable/generated/torch.cat.html#torch-cat">torch.cat()</a> to work later.</p>
<div class="cell" data-execution_count="16">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Convert "fp" column into tensors</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Stack numpy arrays in fingerprint column</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>fp_train_array <span class="op">=</span> np.stack(df_train[<span class="st">"fp"</span>])</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert numpy array data type from uint32 to int32</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>fp_train_array <span class="op">=</span> fp_train_array.astype(<span class="st">"int32"</span>)</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create tensors from array</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>fp_train_t <span class="op">=</span> torch.from_numpy(fp_train_array)</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Reshape tensors</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>fp_train_t <span class="op">=</span> torch.reshape(fp_train_t, (<span class="bu">int</span>(fp_train_array.size<span class="op">/</span><span class="dv">2</span>), <span class="dv">2</span>))</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>fp_train_t.shape <span class="co"># tensor.ndim to check tensor dimensions</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="156">
<pre><code>torch.Size([19456, 2])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>adr_train_t.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="157">
<pre><code>torch.Size([517, 2])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>cyp_str_train_t.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="158">
<pre><code>torch.Size([19, 2])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Concatenate adr tensors, fingerprint tensors and cyp strength tensors as X_train</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> torch.cat([adr_train_t, fp_train_t, cyp_str_train_t], <span class="dv">0</span>).<span class="bu">float</span>()</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>X_train</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="159">
<pre><code>tensor([[ 0.2880, -1.1804],
        [ 0.6725,  0.9416],
        [-1.7143,  0.3996],
        ...,
        [ 1.0000,  0.0000],
        [ 1.0000,  0.0000],
        [ 1.0000,  0.0000]], grad_fn=&lt;CatBackward0&gt;)</code></pre>
</div>
</div>
<p>X_test is being set up similarly as shown below.</p>
<div class="cell" data-execution_count="20">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co">## X_test</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Convert "cyp_strength_of_evidence" into one-hot encodings</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>total_cyp_str_test <span class="op">=</span> <span class="bu">len</span>(<span class="bu">set</span>(df_test[<span class="st">"cyp_strength_of_evidence"</span>]))</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>array_test <span class="op">=</span> df_test[<span class="st">"cyp_strength_of_evidence"</span>].to_numpy()</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>array_test.flags.writeable <span class="op">=</span> <span class="va">True</span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>cyp_str_test_t <span class="op">=</span> one_hot(torch.from_numpy(array_test) <span class="op">%</span> total_cyp_str_test)</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Convert "adverse_drug_reactions" column into embeddings</span></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>adr_str_test <span class="op">=</span> df_test[<span class="st">"adverse_drug_reactions"</span>].tolist()</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>adr_string_test <span class="op">=</span> <span class="st">","</span>.join(adr_str_test)</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>adr_test_t <span class="op">=</span> words_tensors(adr_string_test)</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Convert "fp" column into tensors</span></span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>fp_test_array <span class="op">=</span> np.stack(df_test[<span class="st">"fp"</span>])</span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>fp_test_array <span class="op">=</span> fp_test_array.astype(<span class="st">"int32"</span>)</span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a>fp_test_t <span class="op">=</span> torch.from_numpy(fp_test_array)</span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a>fp_test_t <span class="op">=</span> torch.reshape(fp_test_t, (<span class="bu">int</span>(fp_test_array.size<span class="op">/</span><span class="dv">2</span>),<span class="dv">2</span>))</span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Concatenate adr tensors, drug class tensors and cyp strength tensors as X_test</span></span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> torch.cat([cyp_str_test_t, adr_test_t, fp_test_t], <span class="dv">0</span>).<span class="bu">float</span>()</span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a>X_test</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="160">
<pre><code>tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        ...,
        [0., 0.],
        [0., 0.],
        [0., 0.]], grad_fn=&lt;CatBackward0&gt;)</code></pre>
</div>
</div>
<p>This is followed by setting up y_train.</p>
<div class="cell" data-execution_count="21">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co">## y_train</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Use drug_class column as target</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert "drug_class" column into embeddings </span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="co"># total number of drug classes in df = 20 - len(set(df["drug_class"])) - using embeddings instead of one-hot</span></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>dc_str_train <span class="op">=</span> df_train[<span class="st">"drug_class"</span>].tolist()</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>dc_string_train <span class="op">=</span> <span class="st">","</span>.join(dc_str_train)</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> words_tensors(dc_string_train)</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>y_train</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="161">
<pre><code>tensor([[-1.1321, -0.3473]], grad_fn=&lt;EmbeddingBackward0&gt;)</code></pre>
</div>
</div>
<p>Lastly, y_test is being specified.</p>
<div class="cell" data-execution_count="22">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co">## y_test</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert "drug_class" column into embeddings </span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>dc_str_test <span class="op">=</span> df_test[<span class="st">"drug_class"</span>].tolist()</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>dc_string_test <span class="op">=</span> <span class="st">","</span>.join(dc_str_test)</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> words_tensors(dc_string_test)</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>y_test</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="162">
<pre><code>tensor([[-0.4137, -0.1747]], grad_fn=&lt;EmbeddingBackward0&gt;)</code></pre>
</div>
</div>
<p><br></p>
</section>
<section id="input-preprocessing-pipeline-using-pytorch-dataset-and-dataloader" class="level5">
<h5 class="anchored" data-anchor-id="input-preprocessing-pipeline-using-pytorch-dataset-and-dataloader"><strong>Input preprocessing pipeline using PyTorch Dataset and DataLoader</strong></h5>
<p>There is a size-mismatch-between-tensors warning when I’m trying to use PyTorch’s TensorDataset(). I’ve found out that to use the data loader and tensor dataset, the first dimension of all tensors needs to be the same. Initially, they’re not, where X_train.shape = [24313, 2], y_train.shape = [1, 2]. Eventually I’ve settled on two ways that can help with this:</p>
<ul>
<li><p>use tensor.unsqueeze(dim = 1) or</p></li>
<li><p>use tensor[None] which’ll insert a new dimension at the beginning, then it becomes: X_train.shape = [1, 24313, 2], y_train.shape = [1, 1, 2]</p></li>
</ul>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>X_train[<span class="va">None</span>].shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="163">
<pre><code>torch.Size([1, 19992, 2])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>X_train.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="164">
<pre><code>torch.Size([19992, 2])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>y_train[<span class="va">None</span>].shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="165">
<pre><code>torch.Size([1, 1, 2])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>y_train.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="166">
<pre><code>torch.Size([1, 2])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a PyTorch dataset on training data set</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>train_data <span class="op">=</span> TensorDataset(X_train[<span class="va">None</span>], y_train[<span class="va">None</span>])</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Sets a seed number to generate random numbers</span></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1</span>)</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a dataset loader</span></span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>train_dl <span class="op">=</span> DataLoader(train_data, batch_size, shuffle <span class="op">=</span> <span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create another PyTorch dataset on testing data set</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>test_data <span class="op">=</span> TensorDataset(X_test[<span class="va">None</span>], y_test[<span class="va">None</span>])</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1</span>)</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>test_dl <span class="op">=</span> DataLoader(test_data, batch_size, shuffle<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><br></p>
</section>
<section id="set-up-a-simple-dnn-regression-model" class="level5">
<h5 class="anchored" data-anchor-id="set-up-a-simple-dnn-regression-model"><strong>Set up a simple DNN regression model</strong></h5>
<p>I’m only going to use a very simple two-layer DNN model to match the tiny dataset used here. There are many other types of neural network layers or bits and pieces that can be used to suit the goals and purposes of the dataset used. This <a href="https://pytorch.org/docs/stable/nn.html">reference link</a> shows different types of neural network layers that can be used in PyTorch.</p>
<p>Below are some short notes regarding a neural network (NN) model:</p>
<ul>
<li><p>goal of the model is to minimise loss function L(W) (where W = weight) to get the optimal model weights</p></li>
<li><p>matrix with W (for hidden layer) connects input to hidden layer; matrix with W (for outer layer) connects hidden to output layer</p></li>
<li><p><em>Input layer</em> -&gt; activation function of hidden layer -&gt; <em>hidden layer</em> -&gt; activation function of output layer -&gt; <em>output layer</em> (a very-simplified flow diagram to show how the layers get connected to each other)</p></li>
</ul>
<p>About backpropagation for loss function:</p>
<ul>
<li><p>backpropagation is a computationally efficient way to calculate partial derivatives of loss function to update weights in multi-layer NNs</p></li>
<li><p>it’s based on calculus chain rule to compute derivatives of mathematical functions (automatic differentiation)</p></li>
<li><p>matrix-vector multiplications in backpropagation are computationally more efficient to calculate than matrix-matrix multiplications e.g.&nbsp;forward propagation</p></li>
</ul>
<p>Note: there are also <a href="https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions">other types of activation functions available</a> to use in PyTorch.</p>
<div class="cell" data-execution_count="29">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># note: this is a very simple two-layer NN model only</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up hidden units between two connected layers - one layer with 6 hidden units and the other with 3 hidden units</span></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>hidden_units <span class="op">=</span> [<span class="dv">6</span>, <span class="dv">3</span>]</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Input size same as number of columns in X_train</span></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>input_size <span class="op">=</span> X_train.shape[<span class="dv">1</span>]</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Initiate NN layers as a list</span></span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>all_layers <span class="op">=</span> []</span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a><span class="co">## Specify how the input, hidden and output layers are going to be connected</span></span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a><span class="co"># For each hidden unit within the hidden units specified above:</span></span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> h_unit <span class="kw">in</span> hidden_units:</span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># specify sizes of input sample (input size = X_train col size) &amp; output sample (hidden units) in each layer</span></span>
<span id="cb52-14"><a href="#cb52-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear</span></span>
<span id="cb52-15"><a href="#cb52-15" aria-hidden="true" tabindex="-1"></a>    layer <span class="op">=</span> nn.Linear(input_size, h_unit)</span>
<span id="cb52-16"><a href="#cb52-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># add each layer</span></span>
<span id="cb52-17"><a href="#cb52-17" aria-hidden="true" tabindex="-1"></a>    all_layers.append(layer)</span>
<span id="cb52-18"><a href="#cb52-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># add activation function (trying rectified linear unit) for next layer</span></span>
<span id="cb52-19"><a href="#cb52-19" aria-hidden="true" tabindex="-1"></a>    all_layers.append(nn.ReLU())</span>
<span id="cb52-20"><a href="#cb52-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># for the next layer to be added, the input size will be the same size as the hidden unit</span></span>
<span id="cb52-21"><a href="#cb52-21" aria-hidden="true" tabindex="-1"></a>    input_size <span class="op">=</span> h_unit</span>
<span id="cb52-22"><a href="#cb52-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-23"><a href="#cb52-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Specify the last layer (where input_feature = hidden_units[-1] = 3)</span></span>
<span id="cb52-24"><a href="#cb52-24" aria-hidden="true" tabindex="-1"></a>all_layers.append(nn.Linear(hidden_units[<span class="op">-</span><span class="dv">1</span>], <span class="dv">1</span>))</span>
<span id="cb52-25"><a href="#cb52-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-26"><a href="#cb52-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up a container that'll connect all layers in the specified sequence in the model</span></span>
<span id="cb52-27"><a href="#cb52-27" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(<span class="op">*</span>all_layers)</span>
<span id="cb52-28"><a href="#cb52-28" aria-hidden="true" tabindex="-1"></a>model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="169">
<pre><code>Sequential(
  (0): Linear(in_features=2, out_features=6, bias=True)
  (1): ReLU()
  (2): Linear(in_features=6, out_features=3, bias=True)
  (3): ReLU()
  (4): Linear(in_features=3, out_features=1, bias=True)
)</code></pre>
</div>
</div>
<p><br></p>
</section>
<section id="train-model" class="level5">
<h5 class="anchored" data-anchor-id="train-model"><strong>Train model</strong></h5>
<p>This part is mainly about defining the loss function when training the model with the training data, and optimising model by using a stochastic gradient descent. One key thing I’ve gathered from trying to learn about deep learning is that we’re aiming for global minima and not local minima (e.g.&nbsp;if learning rate is too small, this may end up with local minima; if learning rate is too large, it may end up over-estimating the global minima). I’ve also encountered the PyTorch padding method to make sure the input and target tensors are of the same size, otherwise the model will run into matrix broadcasting issue (which will likely influence the results). The training loss appears to have converged when the epoch runs reach 100 and/or after (note this may vary due to shuffle data sampling)… (I also think my data size is way too small to show a clear contrast in training loss convergence).</p>
<p>References for: <a href="https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss">nn.MSELoss()</a> - measures mean squared error between X and y, and <a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html#torch.nn.functional.pad">nn.functional.pad()</a> - pads tensor (increase tensor size)</p>
<p>Obtaining training loss via model training:</p>
<div class="cell" data-execution_count="30">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up loss function</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>loss_f <span class="op">=</span> nn.MSELoss()</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up stochastic gradient descent optimiser to optimise model (minimise loss) during training </span></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a><span class="co"># lr = learning rate - default: 0.049787 (1*e^-3)</span></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>optim <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.005</span>)</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Set training epochs (epoch: each cycle of training or passing through the training set)</span></span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the log output to show training loss - for every 20 epochs</span></span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>log_epochs <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1</span>)</span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Create empty lists to save training loss (for training and testing/validation sets)</span></span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a>train_epoch_loss <span class="op">=</span> []</span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a>test_epoch_loss <span class="op">=</span> []</span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-15"><a href="#cb54-15" aria-hidden="true" tabindex="-1"></a><span class="co">#  Predict via training X_batch &amp; obtain train loss via loss function from X_batch &amp; y_batch</span></span>
<span id="cb54-16"><a href="#cb54-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb54-17"><a href="#cb54-17" aria-hidden="true" tabindex="-1"></a>    train_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb54-18"><a href="#cb54-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> X_batch, y_batch <span class="kw">in</span> train_dl:</span>
<span id="cb54-19"><a href="#cb54-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Make predictions</span></span>
<span id="cb54-20"><a href="#cb54-20" aria-hidden="true" tabindex="-1"></a>        predict <span class="op">=</span> model(X_batch)[:, <span class="dv">0</span>]</span>
<span id="cb54-21"><a href="#cb54-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Make input tensors the same size as y_batch tensors</span></span>
<span id="cb54-22"><a href="#cb54-22" aria-hidden="true" tabindex="-1"></a>        predict_pad <span class="op">=</span> F.pad(predict[<span class="va">None</span>], pad<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>))</span>
<span id="cb54-23"><a href="#cb54-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate training loss</span></span>
<span id="cb54-24"><a href="#cb54-24" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_f(predict_pad, y_batch)</span>
<span id="cb54-25"><a href="#cb54-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate gradients (backpropagations)</span></span>
<span id="cb54-26"><a href="#cb54-26" aria-hidden="true" tabindex="-1"></a>        loss.backward(retain_graph<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb54-27"><a href="#cb54-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update parameters using gradients</span></span>
<span id="cb54-28"><a href="#cb54-28" aria-hidden="true" tabindex="-1"></a>        optim.step()</span>
<span id="cb54-29"><a href="#cb54-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reset gradients back to zero</span></span>
<span id="cb54-30"><a href="#cb54-30" aria-hidden="true" tabindex="-1"></a>        optim.zero_grad()</span>
<span id="cb54-31"><a href="#cb54-31" aria-hidden="true" tabindex="-1"></a>        train_loss <span class="op">+=</span> loss.item()</span>
<span id="cb54-32"><a href="#cb54-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb54-33"><a href="#cb54-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> epoch <span class="op">%</span> log_epochs <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb54-34"><a href="#cb54-34" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss"> Loss </span><span class="sc">{</span>train_loss<span class="op">/</span><span class="bu">len</span>(train_dl)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb54-35"><a href="#cb54-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-36"><a href="#cb54-36" aria-hidden="true" tabindex="-1"></a>    train_epoch_loss.append(train_loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0 Loss 0.8081</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 20 Loss 0.7587</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 40 Loss 0.7225
Epoch 60 Loss 0.6962
Epoch 80 Loss 0.6776</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 100 Loss 0.6648
Epoch 120 Loss 0.6562</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 140 Loss 0.6505</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 160 Loss 0.6469
Epoch 180 Loss 0.6446</code></pre>
</div>
</div>
<p>Obtaining test or validation loss:</p>
<div class="cell" data-execution_count="31">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict via testing X_batch &amp; obtain test loss </span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> X_batch, y_batch <span class="kw">in</span> test_dl:</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Make predictions</span></span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>        predict_test <span class="op">=</span> model(X_batch)[:, <span class="dv">0</span>]</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Make input tensors the same size as y_batch tensors</span></span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>        predict_pad_test <span class="op">=</span> F.pad(predict_test[<span class="va">None</span>], pad<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>))</span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate training loss</span></span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_f(predict_pad_test, y_batch)</span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate gradients (backpropagations)</span></span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true" tabindex="-1"></a>        loss.backward(retain_graph<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb61-13"><a href="#cb61-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update parameters using gradients</span></span>
<span id="cb61-14"><a href="#cb61-14" aria-hidden="true" tabindex="-1"></a>        optim.step()</span>
<span id="cb61-15"><a href="#cb61-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reset gradients back to zero</span></span>
<span id="cb61-16"><a href="#cb61-16" aria-hidden="true" tabindex="-1"></a>        optim.zero_grad()</span>
<span id="cb61-17"><a href="#cb61-17" aria-hidden="true" tabindex="-1"></a>        test_loss <span class="op">+=</span> loss.item()</span>
<span id="cb61-18"><a href="#cb61-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb61-19"><a href="#cb61-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> epoch <span class="op">%</span> log_epochs <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb61-20"><a href="#cb61-20" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss"> Loss </span><span class="sc">{</span>test_loss<span class="op">/</span><span class="bu">len</span>(test_dl)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb61-21"><a href="#cb61-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-22"><a href="#cb61-22" aria-hidden="true" tabindex="-1"></a>    test_epoch_loss.append(test_loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0 Loss 0.1030</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 20 Loss 0.0963
Epoch 40 Loss 0.0921
Epoch 60 Loss 0.0895
Epoch 80 Loss 0.0880
Epoch 100 Loss 0.0870</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 120 Loss 0.0865</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 140 Loss 0.0861
Epoch 160 Loss 0.0859
Epoch 180 Loss 0.0858</code></pre>
</div>
</div>
<p><br></p>
</section>
<section id="evaluate-model" class="level5">
<h5 class="anchored" data-anchor-id="evaluate-model"><strong>Evaluate model</strong></h5>
<p>Showing train and test losses over training epochs in a plot:</p>
<div class="cell" data-execution_count="32">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>plt.plot(train_epoch_loss, label<span class="op">=</span><span class="st">"train_loss"</span>)</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>plt.plot(test_epoch_loss, label<span class="op">=</span><span class="st">"test_loss"</span>)</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Loss"</span>)</span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a>plt.show</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="172">
<pre><code>&lt;function matplotlib.pyplot.show(close=None, block=None)&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="2_ADR_regressor_files/figure-html/cell-33-output-2.png" width="589" height="429"></p>
</div>
</div>
<p>At the moment, when this notebook is re-run on a refreshed kernel, this leads to a different train and test split each time, and also leading to a different train and test (validation) loss each time. There may be two types of scenarios shown in the plot above where:</p>
<ul>
<li>test loss is higher than train loss (overfitting) - showing the model may be way too simplified and is likely under-trained</li>
<li>train loss is higher than test loss (underfitting) - showing that the model may not have been trained well, and is unable to learn the features in the training data and apply them to the test data</li>
</ul>
<p>When there are actually more training data available with also other hyperparameters fine tuned, it may be possible to see another scenario where both test loss and train loss are very similar in trend, meaning the model is being trained well and able to generalise the training to the unseen data.</p>
<p>To mitigate overfitting:</p>
<ul>
<li><p>firstly there should be more training data than what I’ve had here</p></li>
<li><p>use L1 or L2 regularisation to minimise model complexity by adding penalities to large weights</p></li>
<li><p>use early stopping during model training to stop training the model when test loss is becoming higher than the train loss</p></li>
<li><p>use <a href="https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html">torch.nn.Dropout()</a> to randomly drop out some of the neurons to ensure the exisiting neurons will learn features without being too reliant on other neighbouring neurons in the network</p></li>
<li><p><em>I’ll try the early stopping or drop out method in future posts since current post is relatively long already…</em> (<strong><em><a href="https://jhylin.github.io/Data_in_life_blog/posts/22_Simple_dnn_adrs/4_Prevent_overfit_note.html">note</a> on preventing model overfitting in DNN added in February 2025</em></strong>)</p></li>
</ul>
<p>To overcome underfitting:</p>
<ul>
<li>increase training epochs</li>
<li>minimise regularisation</li>
<li>consider building a more complex or deeper neural network model</li>
</ul>
<p>I’m trying to keep this post simple so have only used mean squared error (MSE) and mean absolute error (MAE) to evaluate the model which has made a prediction on the test set. The smaller the MSE, the less error the model has when making predictions. However this is not the only metric that will determine if a model is optimal for predictions, as I’ve also noticed that every time there’s a different train and test split, the MAE and MSE values will vary too, so it appears that some splits will generate smaller MSE and other splits will lead to larger MSE.</p>
<div class="cell" data-execution_count="33">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="co"># torch.no_grad() - disable gradient calculations to reduce memory usage for inference (also like a decorator)</span></span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>    predict_test <span class="op">=</span> model(X_test.<span class="bu">float</span>())[:, <span class="dv">0</span>]</span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Padding target tensor with set size of [(1, 2)] as input tensor size will vary </span></span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># when notebook is re-run each time due to butina split with sample shuffling</span></span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># so need to pad the target tensor accordingly</span></span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a>    y_test_pad <span class="op">=</span> F.pad(y_test, pad<span class="op">=</span>(predict_test[<span class="va">None</span>].shape[<span class="dv">1</span>] <span class="op">-</span> y_test.shape[<span class="dv">1</span>], <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>))</span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a>    loss_new <span class="op">=</span> loss_f(predict_test[<span class="va">None</span>], y_test_pad)</span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"MSE for test set: </span><span class="sc">{</span>loss_new<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"MAE for test set: </span><span class="sc">{</span>nn<span class="sc">.</span>L1Loss()(predict_test[<span class="va">None</span>], y_test_pad)<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>MSE for test set: 0.0737
MAE for test set: 0.2680</code></pre>
</div>
</div>
<p><br></p>
</section>
<section id="save-model" class="level5">
<h5 class="anchored" data-anchor-id="save-model"><strong>Save model</strong></h5>
<p>One way to save the model is like below.</p>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> <span class="st">"adr_regressor.pt"</span></span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>torch.save(model, path)</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>model_reload <span class="op">=</span> torch.load(path)</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>model_reload.<span class="bu">eval</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="174">
<pre><code>Sequential(
  (0): Linear(in_features=2, out_features=6, bias=True)
  (1): ReLU()
  (2): Linear(in_features=6, out_features=3, bias=True)
  (3): ReLU()
  (4): Linear(in_features=3, out_features=1, bias=True)
)</code></pre>
</div>
</div>
<p><br></p>
</section>
<section id="reload-model" class="level5">
<h5 class="anchored" data-anchor-id="reload-model"><strong>Reload model</strong></h5>
<p>The saved model is reloaded below with a check to make sure the reloaded version is the same as the saved version.</p>
<p>References for: <a href="https://pytorch.org/docs/stable/generated/torch.max.html#torch.max">torch.max</a> and <a href="https://pytorch.org/docs/stable/generated/torch.argmax.html#torch-argmax">torch.argmax</a></p>
<div class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>pred_reload <span class="op">=</span> model_reload(X_test)</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>y_test_rel_pad <span class="op">=</span> F.pad(y_test, pad<span class="op">=</span>(pred_reload[<span class="va">None</span>].shape[<span class="dv">1</span>] <span class="op">-</span> y_test.shape[<span class="dv">1</span>], <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>))</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>correct <span class="op">=</span> (torch.argmax(pred_reload, dim<span class="op">=</span><span class="dv">1</span>) <span class="op">==</span> y_test_rel_pad).<span class="bu">float</span>()</span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> correct.mean()</span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test accuracy: </span><span class="sc">{</span>accuracy<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Test accuracy: 0.9998</code></pre>
</div>
</div>
<p><br></p>
<p>A few things to consider in the end:</p>
<ul>
<li><p>I haven’t done feature standardisation for X_train which is to centre X_train mean and divide by its standard deviation, code may be like this, X_train_normalised = (X_train - np.mean(X_train))/np.std(X_train) (if used on training data, need to apply this to testing data too)</p></li>
<li><p>Training features are certainly too small, however, the main goal of this very first post is to get an overall idea of how to construct a baseline DNN regression model. There are lots of other things that can be done to the ADRs data e.g.&nbsp;adding more drug molecular features and properties. I have essentially only used the initial molecular fingerprints generated when doing the data split to add a bit of molecular aspect in the training dataset.</p></li>
<li><p>I haven’t taken into account the frequencies of words (e.g.&nbsp;same drug classes and same ADR terms across different drugs) in the training and testing data, however, the aim of this first piece of work is also not a semantic analysis in natural language processing so this might not be needed…</p></li>
<li><p>There may be other PyTorch functions that I do not yet know about that will deal with small datasets e.g.&nbsp;perhaps <a href="https://pytorch.org/docs/stable/sparse.html">torch.sparse</a> may be useful?… so this piece is certainly not the only way to do it, but one of the many ways to work with small data</p></li>
</ul>
<p><br></p>
</section>
<section id="acknowledgements" class="level5">
<h5 class="anchored" data-anchor-id="acknowledgements"><strong>Acknowledgements</strong></h5>
<p>I’m very thankful for the existence of these references, websites and reviewer below which have helped me understand (or scratch a small surface of) deep learning and also solve the coding issues mentioned in this post:</p>
<ul>
<li><p><a href="https://discuss.pytorch.org/">PyTorch forums</a></p></li>
<li><p><a href="https://stackoverflow.com/">Stack Overflow</a></p></li>
<li><p>Raschka, Sebastian, Yuxi (Hayden) Liu, and Vahid Mirjalili. 2022. Machine Learning with PyTorch and Scikit-Learn. Birmingham, UK: Packt Publishing.</p></li>
<li><p><a href="https://baoilleach.blogspot.com">Noel O’Boyle</a> for feedback on this post</p></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="jhylin/Data_in_life_blog" data-repo-id="R_kgDOHw7lGQ" data-category="General" data-category-id="DIC_kwDOHw7lGc4CQzQ_" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
      <div class="nav-footer-center">Website made with 💜 in <a href="https://quarto.org/">Quarto</a> by Jennifer HY Lin © 2022 - 2025</div>
  </div>
</footer>



</body></html>