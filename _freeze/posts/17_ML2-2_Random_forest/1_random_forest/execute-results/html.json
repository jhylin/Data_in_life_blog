{
  "hash": "f46eef34aed2b48d0f5329dc73543d70",
  "result": {
    "markdown": "---\ntitle: Random forest\nsubtitle: 'Series 2.2 - model building, feature importances & model evaluations'\nauthor: Jennifer HY Lin\ndate: 2023-11-3\ndraft: true\ncategories:\n  - Machine learning projects\n  - Tree models\n  - Pandas\n  - Scikit-learn\n  - ChEMBL database\n  - Python\nformat: html\nbibliography: references.bib\n---\n\n##### **What is a random forest?**\n\nThe [decision tree model built last time](https://jhylin.github.io/Data_in_life_blog/posts/16_ML2-1_Decision_tree/3_model_build.html) was purely based on one model on its own, which often might not be as accurate or reflective in real-life. To improve the model, we would then think about using the average of multiple models [@breiman1998] to see if this average output would provide a more realistic outcome. This model averaging approach was, in fact, constantly used in our lives - using majority votes in elections or decision-making processes.\n\nThe same model averaging concept was also used in random forest [@breiman2001], which as the name suggested, was composed of many decision trees (models) forming a forest. Each tree model would be making its own model prediction. By accruing multiple predictions since we have multiple trees, the average obtained from these predictions would produce one single result in the end. The advantage of this was that it improved the accuracy of the prediction by reducing variances, and also minimised the problem of overfitting the model if it was purely based on one model only (more details in section 1.11.2.1. Random Forests from [*scikit-learn*](https://scikit-learn.org/stable/modules/ensemble.html#random-forests)).\n\nThe \"random\" part of the random forest was introduced in two ways. The first one was via using bootstrap samples, which was also known as bagging or bootstrap aggregating [@bruce2020], where samples were drawn with replacements within the training datasets for each tree built in the ensemble (also known as the perturb-and-combine technique [@breiman1998]). While bootstrap sampling was happening, randomness was also incorporated at the same time into the training sets. The second way randomness was introduced was by using a random subset of features for splitting at the nodes, or a full set of features could also be used (although this was generally not recommended). The main goal here was to achieve best splits at each node.\n\n<br>\n\n##### **Random forest in *scikit-learn***\n\n*Scikit-learn* had two main types of random forest classes - [ensemble.RandomForestClassifier()](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) and [ensemble.RandomForestRegressor()](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor). When to use which class would depend on the target values. The easiest thing to do was to decide whether the target variables had class labels (binary types or non-continuous variables e.g. yes or no, or other different categories to be assigned) or continuous (numerical) variables, which in this case, if I were to continue using the same dataset from the decision tree series, it would be a continuous variable or feature, pKi, the inhibition constant. \n\nThere were also two other, alternative random forest methods in *scikit-learn*, which were ensemble.RandomTreesEmbedding() and ensemble.ExtraTreesClassifier() or ensemble.ExtraTreesRegressor(). The difference for RandomTreesEmbedding() was that it was an unsupervised method that used data transformations (more details from section 1.11.2.6. on \"Totally Random Trees Embedding\" in [*scikit-learn*](https://scikit-learn.org/stable/modules/ensemble.html#totally-random-trees-embedding)). On the other side, there was also an option to use ExtraTreesClassifier() or ExtraTreesRegressor() to generate extremely randomised trees that would go for another level up in randomness (more deatils in section 1.11.2.2. on Extremely Randomized Trees from [*scikit-learn*](https://scikit-learn.org/stable/modules/ensemble.html#extremely-randomized-trees)). The main difference for this type of random forest was that while there was already a random subset of feature selection used (with an intention to select the most discerning features), more randomness were added on top of this by using purely randomly generated splitting rules for picking features at the nodes.\n\n<br>\n\n##### **Building a random forest regressor model using *scikit-learn***\n\nAs usual, all the required libraries were imported first.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# Add more libraries from below!\nimport pandas as pd\nimport seaborn as sns\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\n# Showing version of *scikit-learn* used\nprint(sklearn.__version__)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.3.2\n```\n:::\n:::\n\n\nImporting dataset that was preprocessed from last time. Data source: [link to decision tree post](https://jhylin.github.io/Data_in_life_blog/posts/16_ML2-1_Decision_tree/1_data_col_prep.html).\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndata = pd.read_csv(\"ache_2d_chembl.csv\")\ndata.drop(columns = [\"Unnamed: 0\"], inplace=True)\n# Preparing data for compounds with max phase as \"null\"\n# Convert max phase with \"NaN\" to \"null\"\ndata[\"max_phase\"].fillna(\"null\", inplace=True)\ndata.head()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/0y/p72zn_cx4vz1lv6zmyd7gkt00000gn/T/ipykernel_8235/6693926.py:5: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'null' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  data[\"max_phase\"].fillna(\"null\", inplace=True)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>molecule_chembl_id</th>\n      <th>pKi</th>\n      <th>max_phase</th>\n      <th>mw</th>\n      <th>fsp3</th>\n      <th>n_lipinski_hba</th>\n      <th>n_lipinski_hbd</th>\n      <th>n_rings</th>\n      <th>n_hetero_atoms</th>\n      <th>n_heavy_atoms</th>\n      <th>...</th>\n      <th>sas</th>\n      <th>n_aliphatic_carbocycles</th>\n      <th>n_aliphatic_heterocyles</th>\n      <th>n_aliphatic_rings</th>\n      <th>n_aromatic_carbocycles</th>\n      <th>n_aromatic_heterocyles</th>\n      <th>n_aromatic_rings</th>\n      <th>n_saturated_carbocycles</th>\n      <th>n_saturated_heterocyles</th>\n      <th>n_saturated_rings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CHEMBL60745</td>\n      <td>8.787812</td>\n      <td>null</td>\n      <td>245.041526</td>\n      <td>0.400000</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>13</td>\n      <td>...</td>\n      <td>3.185866</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>CHEMBL208599</td>\n      <td>10.585027</td>\n      <td>null</td>\n      <td>298.123676</td>\n      <td>0.388889</td>\n      <td>2</td>\n      <td>2</td>\n      <td>4</td>\n      <td>3</td>\n      <td>21</td>\n      <td>...</td>\n      <td>4.331775</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CHEMBL95</td>\n      <td>6.821023</td>\n      <td>4.0</td>\n      <td>198.115698</td>\n      <td>0.307692</td>\n      <td>2</td>\n      <td>2</td>\n      <td>3</td>\n      <td>2</td>\n      <td>15</td>\n      <td>...</td>\n      <td>2.014719</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CHEMBL173309</td>\n      <td>7.913640</td>\n      <td>null</td>\n      <td>694.539707</td>\n      <td>0.666667</td>\n      <td>8</td>\n      <td>0</td>\n      <td>2</td>\n      <td>8</td>\n      <td>50</td>\n      <td>...</td>\n      <td>2.803680</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CHEMBL1128</td>\n      <td>6.698970</td>\n      <td>4.0</td>\n      <td>201.092042</td>\n      <td>0.400000</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>13</td>\n      <td>...</td>\n      <td>3.185866</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 25 columns</p>\n</div>\n```\n:::\n:::\n\n\n<br>\n\n##### **Training/testing splits**\n\nTwo approaches were used, where one was based purely on max phase split (between max phase null and 4), which was used last time in the decision tree series, and the other one was using the same max phase split but with an ImbalancedLearningRegression method added on top of it.\n\n###### **Preparing training data using max phase split**\n\nSetting up X variable first via the dataframe, and converting it into a NumPy array X (no. of samples, no. of features), keeping it the same as how it was in the decision tree series.\n\nNote: It's usually recommended to copy the original data or dataframe for further data manipulations to avoid any unnecessary changes to the original dataset (this was not used in the decision tree posts but since I'm going to use the same set of data again I'd better do this here.)\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# X variables (molecular features)---\n# Make a copy of the original dataframe first\ndata_mp4 = data.copy()\n# Selecting all max phase 4 compounds\ndata_mp4 = data_mp4[data_mp4[\"max_phase\"] == 4]\nprint(data_mp4.shape)\ndata_mp4.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(10, 25)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>molecule_chembl_id</th>\n      <th>pKi</th>\n      <th>max_phase</th>\n      <th>mw</th>\n      <th>fsp3</th>\n      <th>n_lipinski_hba</th>\n      <th>n_lipinski_hbd</th>\n      <th>n_rings</th>\n      <th>n_hetero_atoms</th>\n      <th>n_heavy_atoms</th>\n      <th>...</th>\n      <th>sas</th>\n      <th>n_aliphatic_carbocycles</th>\n      <th>n_aliphatic_heterocyles</th>\n      <th>n_aliphatic_rings</th>\n      <th>n_aromatic_carbocycles</th>\n      <th>n_aromatic_heterocyles</th>\n      <th>n_aromatic_rings</th>\n      <th>n_saturated_carbocycles</th>\n      <th>n_saturated_heterocyles</th>\n      <th>n_saturated_rings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>CHEMBL95</td>\n      <td>6.821023</td>\n      <td>4.0</td>\n      <td>198.115698</td>\n      <td>0.307692</td>\n      <td>2</td>\n      <td>2</td>\n      <td>3</td>\n      <td>2</td>\n      <td>15</td>\n      <td>...</td>\n      <td>2.014719</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CHEMBL1128</td>\n      <td>6.698970</td>\n      <td>4.0</td>\n      <td>201.092042</td>\n      <td>0.400000</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>13</td>\n      <td>...</td>\n      <td>3.185866</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>CHEMBL640</td>\n      <td>6.000000</td>\n      <td>4.0</td>\n      <td>235.168462</td>\n      <td>0.461538</td>\n      <td>4</td>\n      <td>3</td>\n      <td>1</td>\n      <td>4</td>\n      <td>17</td>\n      <td>...</td>\n      <td>1.791687</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>CHEMBL502</td>\n      <td>7.688246</td>\n      <td>4.0</td>\n      <td>379.214744</td>\n      <td>0.458333</td>\n      <td>4</td>\n      <td>0</td>\n      <td>4</td>\n      <td>4</td>\n      <td>28</td>\n      <td>...</td>\n      <td>2.677222</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>131</th>\n      <td>CHEMBL481</td>\n      <td>7.296709</td>\n      <td>4.0</td>\n      <td>586.279135</td>\n      <td>0.515152</td>\n      <td>10</td>\n      <td>1</td>\n      <td>7</td>\n      <td>10</td>\n      <td>43</td>\n      <td>...</td>\n      <td>3.632560</td>\n      <td>0</td>\n      <td>4</td>\n      <td>4</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 25 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Select molecular features for X variable\nX_mp4_df = data_mp4[['mw', 'fsp3', 'n_lipinski_hba', 'n_lipinski_hbd', 'n_rings', 'n_hetero_atoms', 'n_heavy_atoms', 'n_rotatable_bonds', 'n_radical_electrons', 'tpsa', 'qed', 'clogp', 'sas', 'n_aliphatic_carbocycles', 'n_aliphatic_heterocyles', 'n_aliphatic_rings', 'n_aromatic_carbocycles', 'n_aromatic_heterocyles', 'n_aromatic_rings', 'n_saturated_carbocycles', 'n_saturated_heterocyles', 'n_saturated_rings']]\n\nprint(X_mp4_df.shape)\nX_mp4_df.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(10, 22)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mw</th>\n      <th>fsp3</th>\n      <th>n_lipinski_hba</th>\n      <th>n_lipinski_hbd</th>\n      <th>n_rings</th>\n      <th>n_hetero_atoms</th>\n      <th>n_heavy_atoms</th>\n      <th>n_rotatable_bonds</th>\n      <th>n_radical_electrons</th>\n      <th>tpsa</th>\n      <th>...</th>\n      <th>sas</th>\n      <th>n_aliphatic_carbocycles</th>\n      <th>n_aliphatic_heterocyles</th>\n      <th>n_aliphatic_rings</th>\n      <th>n_aromatic_carbocycles</th>\n      <th>n_aromatic_heterocyles</th>\n      <th>n_aromatic_rings</th>\n      <th>n_saturated_carbocycles</th>\n      <th>n_saturated_heterocyles</th>\n      <th>n_saturated_rings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>198.115698</td>\n      <td>0.307692</td>\n      <td>2</td>\n      <td>2</td>\n      <td>3</td>\n      <td>2</td>\n      <td>15</td>\n      <td>0</td>\n      <td>0</td>\n      <td>38.91</td>\n      <td>...</td>\n      <td>2.014719</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>201.092042</td>\n      <td>0.400000</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>13</td>\n      <td>2</td>\n      <td>0</td>\n      <td>20.23</td>\n      <td>...</td>\n      <td>3.185866</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>235.168462</td>\n      <td>0.461538</td>\n      <td>4</td>\n      <td>3</td>\n      <td>1</td>\n      <td>4</td>\n      <td>17</td>\n      <td>6</td>\n      <td>0</td>\n      <td>58.36</td>\n      <td>...</td>\n      <td>1.791687</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>379.214744</td>\n      <td>0.458333</td>\n      <td>4</td>\n      <td>0</td>\n      <td>4</td>\n      <td>4</td>\n      <td>28</td>\n      <td>6</td>\n      <td>0</td>\n      <td>38.77</td>\n      <td>...</td>\n      <td>2.677222</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>131</th>\n      <td>586.279135</td>\n      <td>0.515152</td>\n      <td>10</td>\n      <td>1</td>\n      <td>7</td>\n      <td>10</td>\n      <td>43</td>\n      <td>4</td>\n      <td>0</td>\n      <td>114.20</td>\n      <td>...</td>\n      <td>3.632560</td>\n      <td>0</td>\n      <td>4</td>\n      <td>4</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 22 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Convert X_mp4_df to numpy array\nX_mp4 = X_mp4_df.to_numpy()\nX_mp4\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\narray([[ 1.98115698e+02,  3.07692308e-01,  2.00000000e+00,\n         2.00000000e+00,  3.00000000e+00,  2.00000000e+00,\n         1.50000000e+01,  0.00000000e+00,  0.00000000e+00,\n         3.89100000e+01,  7.06488238e-01,  2.69580000e+00,\n         2.01471913e+00,  1.00000000e+00,  0.00000000e+00,\n         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n         2.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00],\n       [ 2.01092042e+02,  4.00000000e-01,  2.00000000e+00,\n         1.00000000e+00,  1.00000000e+00,  3.00000000e+00,\n         1.30000000e+01,  2.00000000e+00,  0.00000000e+00,\n         2.02300000e+01,  6.08112327e-01, -1.01700000e+00,\n         3.18586632e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n         1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00],\n       [ 2.35168462e+02,  4.61538462e-01,  4.00000000e+00,\n         3.00000000e+00,  1.00000000e+00,  4.00000000e+00,\n         1.70000000e+01,  6.00000000e+00,  0.00000000e+00,\n         5.83600000e+01,  7.31539693e-01,  1.34040000e+00,\n         1.79168720e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n         1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00],\n       [ 3.79214744e+02,  4.58333333e-01,  4.00000000e+00,\n         0.00000000e+00,  4.00000000e+00,  4.00000000e+00,\n         2.80000000e+01,  6.00000000e+00,  0.00000000e+00,\n         3.87700000e+01,  7.47461492e-01,  4.36110000e+00,\n         2.67722173e+00,  1.00000000e+00,  1.00000000e+00,\n         2.00000000e+00,  2.00000000e+00,  0.00000000e+00,\n         2.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n         1.00000000e+00],\n       [ 5.86279135e+02,  5.15151515e-01,  1.00000000e+01,\n         1.00000000e+00,  7.00000000e+00,  1.00000000e+01,\n         4.30000000e+01,  4.00000000e+00,  0.00000000e+00,\n         1.14200000e+02,  3.55955569e-01,  4.09110000e+00,\n         3.63256044e+00,  0.00000000e+00,  4.00000000e+00,\n         4.00000000e+00,  1.00000000e+00,  2.00000000e+00,\n         3.00000000e+00,  0.00000000e+00,  2.00000000e+00,\n         2.00000000e+00],\n       [ 5.10461822e+02,  8.00000000e-01,  6.00000000e+00,\n         0.00000000e+00,  1.00000000e+00,  6.00000000e+00,\n         3.60000000e+01,  2.10000000e+01,  0.00000000e+00,\n         2.76900000e+01,  2.05822189e-01,  5.45250000e+00,\n         3.25765349e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n         1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00],\n       [ 1.84066459e+02,  1.00000000e+00,  3.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  5.00000000e+00,\n         1.10000000e+01,  4.00000000e+00,  0.00000000e+00,\n         3.55300000e+01,  6.29869319e-01,  2.91400000e+00,\n         3.34514393e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00],\n       [ 2.87152144e+02,  5.29411765e-01,  4.00000000e+00,\n         1.00000000e+00,  4.00000000e+00,  4.00000000e+00,\n         2.10000000e+01,  1.00000000e+00,  0.00000000e+00,\n         4.19300000e+01,  8.00524269e-01,  1.85030000e+00,\n         4.22684283e+00,  1.00000000e+00,  2.00000000e+00,\n         3.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n         1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00],\n       [ 3.48142697e+02,  3.68421053e-01,  2.00000000e+00,\n         0.00000000e+00,  3.00000000e+00,  4.00000000e+00,\n         2.30000000e+01,  5.00000000e+00,  0.00000000e+00,\n         6.48000000e+00,  7.09785317e-01,  5.44140000e+00,\n         4.22359068e+00,  0.00000000e+00,  1.00000000e+00,\n         1.00000000e+00,  2.00000000e+00,  0.00000000e+00,\n         2.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00],\n       [ 2.34092376e+02,  3.07692308e-01,  2.00000000e+00,\n         2.00000000e+00,  3.00000000e+00,  3.00000000e+00,\n         1.60000000e+01,  0.00000000e+00,  0.00000000e+00,\n         3.89100000e+01,  7.60853221e-01,  3.11760000e+00,\n         3.21871482e+00,  1.00000000e+00,  0.00000000e+00,\n         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n         2.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00]])\n```\n:::\n:::\n\n\nAgain, setting up y variable via the dataframe as well, and then converting it into a NumPy array y (no. of samples or target values) - also keeping this the same as the one from the decision tree series.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# y variable (target outcome - pKi)\ny_mp4_df = data_mp4[\"pKi\"]\ny_mp4_df\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n2      6.821023\n4      6.698970\n6      6.000000\n9      7.688246\n131    7.296709\n133    4.431798\n160    5.221849\n171    6.522879\n180    4.607303\n195    6.995679\nName: pKi, dtype: float64\n```\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Convert y_mp4_df to numpy array\ny_mp4 = y_mp4_df.to_numpy()\ny_mp4\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\narray([6.82102305, 6.69897   , 6.        , 7.68824614, 7.29670862,\n       4.43179828, 5.22184875, 6.52287875, 4.60730305, 6.99567863])\n```\n:::\n:::\n\n\n<br>\n\n###### **Training model with the training dataset using max phase split only** \n\nFollowed by fitting RandomForestRegressor() on these X and y variables.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# n_estimators = 100 by default\n# note: if wanting to use whole dataset - switch off \"bootstrap\" parameter by using \"False\"\nrfreg = RandomForestRegressor(max_depth=3, random_state=1)\nrfreg.fit(X_mp4, y_mp4)\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=3, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=3, random_state=1)</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\n<br>\n\n###### **Preparing testing data using max phase split only**\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# Compounds with max phase as \"null\"\ndata_mp_null = data.copy()\n# Selecting all max phase \"null\" compounds\ndata_mp_null = data_mp_null[data_mp_null[\"max_phase\"] == \"null\"]\nprint(data_mp_null.shape)\ndata_mp_null.head() \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(466, 25)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=9}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>molecule_chembl_id</th>\n      <th>pKi</th>\n      <th>max_phase</th>\n      <th>mw</th>\n      <th>fsp3</th>\n      <th>n_lipinski_hba</th>\n      <th>n_lipinski_hbd</th>\n      <th>n_rings</th>\n      <th>n_hetero_atoms</th>\n      <th>n_heavy_atoms</th>\n      <th>...</th>\n      <th>sas</th>\n      <th>n_aliphatic_carbocycles</th>\n      <th>n_aliphatic_heterocyles</th>\n      <th>n_aliphatic_rings</th>\n      <th>n_aromatic_carbocycles</th>\n      <th>n_aromatic_heterocyles</th>\n      <th>n_aromatic_rings</th>\n      <th>n_saturated_carbocycles</th>\n      <th>n_saturated_heterocyles</th>\n      <th>n_saturated_rings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CHEMBL60745</td>\n      <td>8.787812</td>\n      <td>null</td>\n      <td>245.041526</td>\n      <td>0.400000</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>13</td>\n      <td>...</td>\n      <td>3.185866</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>CHEMBL208599</td>\n      <td>10.585027</td>\n      <td>null</td>\n      <td>298.123676</td>\n      <td>0.388889</td>\n      <td>2</td>\n      <td>2</td>\n      <td>4</td>\n      <td>3</td>\n      <td>21</td>\n      <td>...</td>\n      <td>4.331775</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CHEMBL173309</td>\n      <td>7.913640</td>\n      <td>null</td>\n      <td>694.539707</td>\n      <td>0.666667</td>\n      <td>8</td>\n      <td>0</td>\n      <td>2</td>\n      <td>8</td>\n      <td>50</td>\n      <td>...</td>\n      <td>2.803680</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>CHEMBL102226</td>\n      <td>4.698970</td>\n      <td>null</td>\n      <td>297.152928</td>\n      <td>0.923077</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>18</td>\n      <td>...</td>\n      <td>2.965170</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>CHEMBL103873</td>\n      <td>5.698970</td>\n      <td>null</td>\n      <td>269.121628</td>\n      <td>0.909091</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>16</td>\n      <td>...</td>\n      <td>3.097106</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 25 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# Set up X test variable with the same molecular features\nX_mp_test_df = data_mp_null[['mw', 'fsp3', 'n_lipinski_hba', 'n_lipinski_hbd', 'n_rings', 'n_hetero_atoms', 'n_heavy_atoms', 'n_rotatable_bonds', 'n_radical_electrons', 'tpsa', 'qed', 'clogp', 'sas', 'n_aliphatic_carbocycles', 'n_aliphatic_heterocyles', 'n_aliphatic_rings', 'n_aromatic_carbocycles', 'n_aromatic_heterocyles', 'n_aromatic_rings', 'n_saturated_carbocycles', 'n_saturated_heterocyles', 'n_saturated_rings']]\n\n# Convert X test variables from df to arrays\nX_mp_test = X_mp_test_df.to_numpy()\n\nX_mp_test\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\narray([[2.45041526e+02, 4.00000000e-01, 2.00000000e+00, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [2.98123676e+02, 3.88888889e-01, 2.00000000e+00, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [6.94539707e+02, 6.66666667e-01, 8.00000000e+00, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       ...,\n       [3.11152144e+02, 3.15789474e-01, 4.00000000e+00, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [3.68096076e+02, 9.23076923e-01, 4.00000000e+00, ...,\n        0.00000000e+00, 2.00000000e+00, 2.00000000e+00],\n       [2.46136828e+02, 5.00000000e-01, 4.00000000e+00, ...,\n        0.00000000e+00, 3.00000000e+00, 3.00000000e+00]])\n```\n:::\n:::\n\n\n<br>\n\n##### **Training/testing splits using ImbalancedLearningRegression and max phase splits**\n\nI didn't really pay a lot of attentions when I was doing data splits in the last series on decision tree, as my main focus was on building a single tree in order to fully understand and see what could be derived from just one tree. Now, when I reached this series on random forest, I realised I forgot to mention in the last series that data splitting was actually very crucial and impactful on model performance and also outcome predictions. It could also become quite complicated as more approaches were available to split the data, and the way the data was splitted could produce different outcomes. After I've splitted the same dataset based on compounds' max phase assignments and also fitted the training data on the random forest regressor, I went back and had another look, then noticed that, the training and testing data were very imbalanced, and perhaps I should do something to the data before fitting them onto the estimator. \n\nSo I started a simple online search, and after several quick online and also text book reads, I've decided that the likely plan for this series on random forest was to stick with max phase splits for now, since this was a regression problem (as I was trying to predict a continuous variable, inhibition constant, pKi, and not a binary label or outcome). Based on current common ML concensus (Google, StackOverflow, ML mastery examples), imbalanced dataset was more applicable to classification tasks (e.g. binary labels or multi-class labels), rather than regression problems. However, with more current ML researches looking into the issue of imbalanced datasets, there were actually findings using other strategies such as deep imbalanced regression for regression problems too (cite bookmarked paper, there might be more studies in this area as well).\n\n* Try ImbalancedLearningRegression (https://imbalancedlearningregression.readthedocs.io/en/latest/intro.html)\n- try random over-sampling first (potentially other variations of shorter posts later on using SMOTE or Introduction of Gaussian Noise to see if leading to different results)\n\nSMOTE -> SMOTER -> SMOGN -> ImbalancedLearningRegression\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n# Original dataset\nprint(data.shape)\ndata.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(481, 25)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>molecule_chembl_id</th>\n      <th>pKi</th>\n      <th>max_phase</th>\n      <th>mw</th>\n      <th>fsp3</th>\n      <th>n_lipinski_hba</th>\n      <th>n_lipinski_hbd</th>\n      <th>n_rings</th>\n      <th>n_hetero_atoms</th>\n      <th>n_heavy_atoms</th>\n      <th>...</th>\n      <th>sas</th>\n      <th>n_aliphatic_carbocycles</th>\n      <th>n_aliphatic_heterocyles</th>\n      <th>n_aliphatic_rings</th>\n      <th>n_aromatic_carbocycles</th>\n      <th>n_aromatic_heterocyles</th>\n      <th>n_aromatic_rings</th>\n      <th>n_saturated_carbocycles</th>\n      <th>n_saturated_heterocyles</th>\n      <th>n_saturated_rings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CHEMBL60745</td>\n      <td>8.787812</td>\n      <td>null</td>\n      <td>245.041526</td>\n      <td>0.400000</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>13</td>\n      <td>...</td>\n      <td>3.185866</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>CHEMBL208599</td>\n      <td>10.585027</td>\n      <td>null</td>\n      <td>298.123676</td>\n      <td>0.388889</td>\n      <td>2</td>\n      <td>2</td>\n      <td>4</td>\n      <td>3</td>\n      <td>21</td>\n      <td>...</td>\n      <td>4.331775</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CHEMBL95</td>\n      <td>6.821023</td>\n      <td>4.0</td>\n      <td>198.115698</td>\n      <td>0.307692</td>\n      <td>2</td>\n      <td>2</td>\n      <td>3</td>\n      <td>2</td>\n      <td>15</td>\n      <td>...</td>\n      <td>2.014719</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CHEMBL173309</td>\n      <td>7.913640</td>\n      <td>null</td>\n      <td>694.539707</td>\n      <td>0.666667</td>\n      <td>8</td>\n      <td>0</td>\n      <td>2</td>\n      <td>8</td>\n      <td>50</td>\n      <td>...</td>\n      <td>2.803680</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CHEMBL1128</td>\n      <td>6.698970</td>\n      <td>4.0</td>\n      <td>201.092042</td>\n      <td>0.400000</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>13</td>\n      <td>...</td>\n      <td>3.185866</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 25 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\n# Trial using ImbalancedLearningRegression\nimport ImbalancedLearningRegression as iblr\n\ndata = data.copy()\n\n# Using random over-sampling on the original dataset (pre-max phase split)\n# data_ro = iblr.ro(data = data, y = \"pKi\")\n# print(data_ro.shape)\n```\n:::\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n# Gaussian noise\ndata_gn = iblr.gn(data = data, y = \"pKi\", pert = 1)\nprint(data_gn.shape)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\rsynth_matrix:   0%|          | 0/58 [00:00<?, ?it/s]\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\rsynth_matrix:  17%|#7        | 10/58 [00:00<00:00, 91.75it/s]\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\rsynth_matrix:  34%|###4      | 20/58 [00:00<00:00, 93.91it/s]\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\rsynth_matrix:  52%|#####1    | 30/58 [00:00<00:00, 95.00it/s]\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\rsynth_matrix:  69%|######8   | 40/58 [00:00<00:00, 94.80it/s]\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\rsynth_matrix:  86%|########6 | 50/58 [00:00<00:00, 94.53it/s]\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\rsynth_matrix: 100%|##########| 58/58 [00:00<00:00, 94.69it/s]\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\rr_index:   0%|          | 0/8 [00:00<?, ?it/s]\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\rr_index: 100%|##########| 8/8 [00:00<00:00, 293.32it/s]\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(480, 25)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\n```\n:::\n:::\n\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\n# Gaussian Noise\n# Followed by max phase split, where 4 = training\ndata_gn_mp4 = data_gn[data_gn[\"max_phase\"] == 4]\ndata_gn_mp4\nprint(data_gn_mp4.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(4, 25)\n```\n:::\n:::\n\n\nRandom over-sampling keeps all 10 max phase 4 compounds.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\n# Random over-sampling\n# Followed by max phase split, where 4 = training\n# data_ro_mp4 = data_ro[data_ro[\"max_phase\"] == 4]\n# data_ro_mp4\n# print(data_ro_mp4.shape)\n```\n:::\n\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\n# Gaussian Noise\n# Also splitted max phase null compounds = testing\ndata_gn_mp_null = data_gn[data_gn[\"max_phase\"] == \"null\"]\ndata_gn_mp_null\n# Show shape of df for max phase null compounds\nprint(data_gn_mp_null.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(470, 25)\n```\n:::\n:::\n\n\nRandom over-sampling is actually oversampling the max phase null compounds (so number increased).\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\n# Random over-sampling\n# # Also splitted max phase null compounds = testing\n# data_ro_mp_null = data_ro[data_ro[\"max_phase\"] == \"null\"]\n# data_ro_mp_null\n# # Show shape of df for max phase null compounds\n# print(data_ro_mp_null.shape)\n```\n:::\n\n\nTested over-sampling - essentially it increased the sample size of all max phase null compounds with all 10 max phase 4 compounds kept.\n\nTested under-sampling - removed all of the max phase 4 compounds (therefore not ideal), with max phase null compounds also reduced in size too.\n\nDecided to stick with Gauissian Noise version as it reduced both max phase 4 compounds slightly, and increased the max phase null compounds slightly as well. It appeared to be the most balanced adjustments for both max phase null and 4 compounds.\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\n# Quick look at how the pKi values differed \n# after applying iblr to dataset\n\nimport matplotlib.pyplot as plt\n\n# Plot target variable distributions\nsns.kdeplot(data[\"pKi\"], label = \"Original\")\nsns.kdeplot(data_gn[\"pKi\"], label = \"Modified\")\nplt.legend(labels = [\"Original\", \"Modified\"])\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\n<matplotlib.legend.Legend at 0x13369eb00>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](1_random_forest_files/figure-html/cell-19-output-2.png){width=597 height=429}\n:::\n:::\n\n\nRange of pKi values for max phase 4 compounds was between 4 and 8.\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\n# Checking pKi value range for max phase 4 compounds\n#data_mp4\n```\n:::\n\n\nPreparing training data.\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\n# Select molecular features for X variable\nX_mp4_gn_df = data_gn_mp4[['mw', 'fsp3', 'n_lipinski_hba', 'n_lipinski_hbd', 'n_rings', 'n_hetero_atoms', 'n_heavy_atoms', 'n_rotatable_bonds', 'n_radical_electrons', 'tpsa', 'qed', 'clogp', 'sas', 'n_aliphatic_carbocycles', 'n_aliphatic_heterocyles', 'n_aliphatic_rings', 'n_aromatic_carbocycles', 'n_aromatic_heterocyles', 'n_aromatic_rings', 'n_saturated_carbocycles', 'n_saturated_heterocyles', 'n_saturated_rings']]\n\nprint(X_mp4_gn_df.shape)\nX_mp4_gn_df.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(4, 22)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=20}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mw</th>\n      <th>fsp3</th>\n      <th>n_lipinski_hba</th>\n      <th>n_lipinski_hbd</th>\n      <th>n_rings</th>\n      <th>n_hetero_atoms</th>\n      <th>n_heavy_atoms</th>\n      <th>n_rotatable_bonds</th>\n      <th>n_radical_electrons</th>\n      <th>tpsa</th>\n      <th>...</th>\n      <th>sas</th>\n      <th>n_aliphatic_carbocycles</th>\n      <th>n_aliphatic_heterocyles</th>\n      <th>n_aliphatic_rings</th>\n      <th>n_aromatic_carbocycles</th>\n      <th>n_aromatic_heterocyles</th>\n      <th>n_aromatic_rings</th>\n      <th>n_saturated_carbocycles</th>\n      <th>n_saturated_heterocyles</th>\n      <th>n_saturated_rings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>285</th>\n      <td>198.115698</td>\n      <td>0.307692</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>15.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>38.91</td>\n      <td>...</td>\n      <td>2.014719</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>286</th>\n      <td>348.142697</td>\n      <td>0.368421</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>23.0</td>\n      <td>5.0</td>\n      <td>0</td>\n      <td>6.48</td>\n      <td>...</td>\n      <td>4.223591</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>287</th>\n      <td>586.279135</td>\n      <td>0.515152</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>7.0</td>\n      <td>10.0</td>\n      <td>43.0</td>\n      <td>4.0</td>\n      <td>0</td>\n      <td>114.20</td>\n      <td>...</td>\n      <td>3.632560</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>337</th>\n      <td>201.092042</td>\n      <td>0.400000</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>13.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>20.23</td>\n      <td>...</td>\n      <td>3.185866</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>4 rows × 22 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\nX_mp4_gn = X_mp4_gn_df.to_numpy()\n```\n:::\n\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\n# y variable (target outcome - pKi)\ny_mp4_gn_df = data_gn_mp4[\"pKi\"]\n\ny_mp4_gn = y_mp4_gn_df.to_numpy()\ny_mp4_gn\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\narray([6.82102305, 4.60730305, 7.29670862, 6.69897   ])\n```\n:::\n:::\n\n\nFitting iblr-Gaussian noise training data onto another random forest regressor model.\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\n# n_estimators = 100 by default\n# note: if wanting to use whole dataset - switch off \"bootstrap\" parameter by using \"False\"\nrfreg_gn = RandomForestRegressor(max_depth=3, random_state=1)\nrfreg_gn.fit(X_mp4_gn, y_mp4_gn)\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```{=html}\n<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=3, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=3, random_state=1)</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\nPreparing testing data.\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\n# Set up X test variable with the same molecular features\nX_mp_gn_test_df = data_gn_mp_null[['mw', 'fsp3', 'n_lipinski_hba', 'n_lipinski_hbd', 'n_rings', 'n_hetero_atoms', 'n_heavy_atoms', 'n_rotatable_bonds', 'n_radical_electrons', 'tpsa', 'qed', 'clogp', 'sas', 'n_aliphatic_carbocycles', 'n_aliphatic_heterocyles', 'n_aliphatic_rings', 'n_aromatic_carbocycles', 'n_aromatic_heterocyles', 'n_aromatic_rings', 'n_saturated_carbocycles', 'n_saturated_heterocyles', 'n_saturated_rings']]\n\n# Convert X test variables from df to arrays\nX_mp_gn_test = X_mp_gn_test_df.to_numpy()\n\nX_mp_gn_test\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```\narray([[5.80224119e+02, 2.64705882e-01, 7.00000000e+00, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [3.77210327e+02, 3.91304348e-01, 5.00000000e+00, ...,\n        0.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n       [5.24297368e+02, 4.54545455e-01, 4.00000000e+00, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       ...,\n       [2.88072177e+02, 2.50000000e-01, 5.00000000e+00, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [3.67214744e+02, 4.34782609e-01, 4.00000000e+00, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [3.51169525e+02, 3.68421053e-01, 7.00000000e+00, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])\n```\n:::\n:::\n\n\n<br>\n\n###### **Using trained model for prediction on testing data**\n\nPredicting max phase-splitted data only.\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\n# Predict pKi values for the compounds with \"null\" max phase\n# using the training model rfreg \n# Uncomment code below to print prediction result\n#print(rfreg.predict(X_mp_test))\n\n# or use:\ny_mp_test = rfreg.predict(X_mp_test)\n```\n:::\n\n\nPredicting iblr-gn data with max phase splits.\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\ny_mp_gn_test = rfreg_gn.predict(X_mp_gn_test)\n```\n:::\n\n\n<br>\n\n###### **Accuracy/scoring metrics of trained models**\n\nChecking model accuracy for both training and testing datasets was actually recommended to occur before moving onto finding out the feature importances. A *scikit-learn* explanation for this could be found in the section on [\"Permutation feature importance\"](https://scikit-learn.org/stable/modules/permutation_importance.html#permutation-feature-importance).\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\n# Training set accuracy\nprint(f\"Random forest regressor training accuracy: {rfreg.score(X_mp4, y_mp4):.2f}\")\n\n# Testing set accuracy\nprint(f\"Random forest regressor testing accuracy: {rfreg.score(X_mp_test, y_mp_test):.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRandom forest regressor training accuracy: 0.82\nRandom forest regressor testing accuracy: 1.00\n```\n:::\n:::\n\n\nSo it looked like both the training and testing accuracies for the random forest regressor model (rfreg) were quite high, meaning that the model was able to remember the molecular features well from the training set (the tiny sample of 10 compounds), and the model was able to apply them to the testing set (which should contain about 400s of compounds) as well, in order to make predictions on the target value of pKi. So this has confirmed that the model was indeed making predictions (rather than not making any at all, which meant there might be no point in finding out which features were important in the data, so it was a good checking point during the random forest model building exercise), therefore, we could proceed to the next step of generating some feature importances, which were useful information to fill in the bigger story i.e. which features were pivotal for influencing the pKi values of prescription drugs targeting AChE? \n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\n# iblr-Gaussian noise & max phase splitted data\n# Training set accuracy\nprint(f\"Random forest regressor training accuracy: {rfreg_gn.score(X_mp4_gn, y_mp4_gn):.2f}\")\n\n# Testing set accuracy\nprint(f\"Random forest regressor testing accuracy: {rfreg_gn.score(X_mp_gn_test, y_mp_gn_test):.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRandom forest regressor training accuracy: 0.80\nRandom forest regressor testing accuracy: 1.00\n```\n:::\n:::\n\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\ndata_mp_null.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>molecule_chembl_id</th>\n      <th>pKi</th>\n      <th>max_phase</th>\n      <th>mw</th>\n      <th>fsp3</th>\n      <th>n_lipinski_hba</th>\n      <th>n_lipinski_hbd</th>\n      <th>n_rings</th>\n      <th>n_hetero_atoms</th>\n      <th>n_heavy_atoms</th>\n      <th>...</th>\n      <th>sas</th>\n      <th>n_aliphatic_carbocycles</th>\n      <th>n_aliphatic_heterocyles</th>\n      <th>n_aliphatic_rings</th>\n      <th>n_aromatic_carbocycles</th>\n      <th>n_aromatic_heterocyles</th>\n      <th>n_aromatic_rings</th>\n      <th>n_saturated_carbocycles</th>\n      <th>n_saturated_heterocyles</th>\n      <th>n_saturated_rings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CHEMBL60745</td>\n      <td>8.787812</td>\n      <td>null</td>\n      <td>245.041526</td>\n      <td>0.400000</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>13</td>\n      <td>...</td>\n      <td>3.185866</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>CHEMBL208599</td>\n      <td>10.585027</td>\n      <td>null</td>\n      <td>298.123676</td>\n      <td>0.388889</td>\n      <td>2</td>\n      <td>2</td>\n      <td>4</td>\n      <td>3</td>\n      <td>21</td>\n      <td>...</td>\n      <td>4.331775</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CHEMBL173309</td>\n      <td>7.913640</td>\n      <td>null</td>\n      <td>694.539707</td>\n      <td>0.666667</td>\n      <td>8</td>\n      <td>0</td>\n      <td>2</td>\n      <td>8</td>\n      <td>50</td>\n      <td>...</td>\n      <td>2.803680</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>CHEMBL102226</td>\n      <td>4.698970</td>\n      <td>null</td>\n      <td>297.152928</td>\n      <td>0.923077</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>18</td>\n      <td>...</td>\n      <td>2.965170</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>CHEMBL103873</td>\n      <td>5.698970</td>\n      <td>null</td>\n      <td>269.121628</td>\n      <td>0.909091</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>16</td>\n      <td>...</td>\n      <td>3.097106</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 25 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\n#data_mp_null = data_mp_null[data_mp_null[\"max_phase\"] == \"null\"]\ny_true = data_mp_null[\"pKi\"]\ny_true = y_true.to_numpy(copy=True)\n```\n:::\n\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\ntype(y_true)\ny_true\n```\n\n::: {.cell-output .cell-output-display execution_count=31}\n```\narray([ 8.7878124 , 10.58502665,  7.91364017,  4.69897   ,  5.69897   ,\n        7.66354027,  5.14630179,  8.82681373,  6.04866248,  5.63827216,\n        5.82390874,  5.85387196,  6.4436975 ,  6.69897   ,  6.52287875,\n        6.69897   ,  6.88605665,  6.4436975 ,  6.60205999,  6.82390874,\n        6.79588002,  6.79588002,  6.79588002,  6.76955108,  7.2600323 ,\n        8.11002636,  8.24003014,  8.14002156,  8.15002809,  8.55005901,\n        7.84985784,  9.10001544,  7.7000571 ,  7.29998894,  7.        ,\n        8.05998184,  6.05000046,  5.58999929,  6.49000491,  7.        ,\n        6.65999238,  6.91998715,  9.24003014,  9.01999663,  9.32002731,\n        8.43997375,  6.59999765,  8.10001544,  7.36997915,  6.41999931,\n        6.95999136,  6.91998715,  6.2799979 ,  4.70999991,  5.57000078,\n        7.28000617,  7.48999149,  7.55005901,  7.10001544,  6.40999458,\n        6.        ,  6.05000046,  6.54999737,  6.65999238,  6.95999136,\n        7.40000782,  8.15002809,  8.59006688,  6.        ,  8.21997087,\n        8.34998405,  7.        ,  7.3799681 ,  6.85999505,  7.29998894,\n        6.91998715,  7.65995268,  8.07998108,  8.02997873,  7.59006688,\n        6.09000028,  6.47000533,  6.71999105,  7.04997609,  6.11999861,\n        4.51999996,  7.27002571,  5.58999929,  7.51999306,  8.33998878,\n        6.62000596,  6.01999663,  5.77728353,  6.7878124 ,  7.78251606,\n        8.19382003,  7.80410035, 10.48148606,  4.31695296,  5.48412616,\n        6.98296666,  7.91364017,  8.24641694, 10.85387196, 10.82390874,\n       10.69897   ,  6.07058107,  6.04431225,  6.49485002,  4.19111413,\n        8.58502665,  5.71942163,  5.19928292,  4.95116991,  5.38404995,\n        5.65482238,  5.47755577,  5.09854168,  4.46813805,  6.88605665,\n        6.55284197,  7.81247928,  8.2069084 ,  8.17069623,  7.5543958 ,\n        8.        ,  8.7878124 ,  8.12784373,  7.87712908,  9.11463878,\n        8.76497684,  9.86678054,  9.92408824,  9.79048499, 10.92811799,\n        9.65169514,  8.57316355,  7.68026951,  8.28819277,  8.3990271 ,\n        7.87386859,  9.26760624, 10.07211659,  6.9625735 ,  8.71896663,\n        4.06752624,  4.03245202,  4.4225082 ,  4.61978876,  4.11918641,\n        7.21395879,  4.04914854,  5.02872415,  4.7878124 ,  6.09151498,\n        4.53165267,  6.20760831,  7.69897   ,  6.85387196,  7.61978876,\n        6.06550155,  6.46852108,  6.36653154,  6.40893539,  6.26760624,\n        6.01322827,  7.04575749,  7.        ,  8.30103   ,  6.20760831,\n        5.90308999,  7.39794001,  5.63827216,  8.45593196,  8.99567863,\n        4.46470588,  4.16621563,  4.7721133 ,  4.02456819,  4.5214335 ,\n        5.07572071,  5.19791074,  7.60730305,  7.32239305,  7.06803389,\n        8.42829117,  9.49757288,  7.11861534,  7.32790214,  7.09151498,\n        8.18909572,  8.02687215,  7.35008128,  7.33903971,  7.27572413,\n        6.90752499,  6.70381763,  5.84254323,  5.95078198,  5.50863831,\n        5.50863831,  5.50863831,  5.50863831,  5.50863831,  5.50863831,\n        5.50863831,  5.50863831,  5.50863831,  5.5421181 ,  6.0473383 ,\n        6.88588964,  7.64244628,  7.0209071 ,  7.32477175,  7.67182434,\n        7.26090255,  6.61773074,  5.62893214,  6.28287895,  5.68402965,\n        7.21183163,  6.57979663,  7.13206135,  7.22665246,  6.11201719,\n        7.65208481,  5.98296666,  7.66434155,  6.03703906,  6.29518053,\n        6.79309321,  6.27984897,  7.02131743,  7.14935376,  6.70226137,\n        6.89705203,  6.34375802,  6.62613213,  6.03639159,  6.62800909,\n        6.20992605,  7.20586064,  6.64217946,  7.15826522,  7.10980261,\n        7.20718823,  6.66328017,  6.28899464,  6.9279708 ,  6.10065015,\n        6.97363019,  6.39464107,  7.41816408,  6.57444948,  6.39485663,\n        6.58989792,  6.86128689,  6.29663123,  5.87614836,  5.8728952 ,\n        6.06296883,  5.79860288,  6.26482442,  6.68033191,  6.39825499,\n        5.99826629,  7.59516628,  7.68613278,  7.38404995,  8.1426675 ,\n        7.61261017,  7.51999306,  7.51855737,  4.07675598,  4.70996539,\n        5.19314197,  5.70996539,  7.69897   ,  6.39794001,  8.76955108,\n        8.82973828,  8.29929628,  8.71669877,  9.63827216,  9.1079054 ,\n        5.69680394,  7.13667714,  5.63078414,  5.20627944,  5.82623118,\n        5.5118725 ,  6.23054882,  4.5055671 ,  4.2582183 ,  8.57675413,\n        7.10679325,  7.04048162,  7.17979854,  7.29499204,  7.25806092,\n        7.37468755,  7.22475374,  7.25103714,  7.5214335 ,  7.61439373,\n        7.51004152,  7.77728353,  7.11861534,  7.15490196,  7.44611697,\n        7.40450378,  7.35359627,  7.46344156,  7.09474395,  6.70114692,\n        8.38721614,  5.71219827,  5.627088  ,  5.09745322,  5.60380065,\n        5.09854168,  5.93181414, 10.64016452, 10.9788107 , 10.41793664,\n       10.09691001,  7.29998894,  7.3053948 ,  6.85387196,  9.1426675 ,\n        9.00877392,  9.1739252 ,  8.97469413,  9.34678749,  8.9788107 ,\n        9.08092191,  8.79860288,  8.87614836,  8.75945075,  8.8728952 ,\n        8.91364017,  9.1426675 ,  6.00877392,  4.4015378 ,  4.73992861,\n        5.54944299,  5.54944299,  5.37427609,  6.44977165,  5.70333481,\n        7.45593196,  6.58502665,  5.73636393,  7.537602  ,  6.25649024,\n        4.76371472,  5.3635121 ,  5.23210238,  7.82390874,  6.28399666,\n        5.95546024,  6.63264408,  7.50863831,  6.31875876,  6.06048075,\n        5.05060999,  5.57511836,  4.87942607,  5.39685563,  5.42596873,\n        5.47625353,  5.19178903,  4.22329882,  4.92811799,  4.25026368,\n        4.00877392,  4.64206515,  7.4301581 ,  5.41116827,  6.83268267,\n        8.00656377,  8.48412616,  4.37675071,  4.4436975 ,  4.61978876,\n        4.67778071,  4.92081875,  4.65757732,  4.60205999,  4.13667714,\n        4.04575749,  6.7212464 ,  7.95860731,  7.24412514,  7.92081875,\n        7.20760831,  7.40893539,  7.95860731,  7.67778071,  7.22184875,\n        5.49485002,  5.85387196,  5.13135556,  7.79588002,  6.20065945,\n        6.91721463,  6.05601112,  7.21616786,  7.21616786,  7.21616786,\n        7.21616786,  7.21616786,  7.05305673,  8.43062609,  7.77728353,\n        6.88941029,  7.        ,  7.71896663,  7.02965312,  7.52287875,\n        4.29242982,  6.76955108,  7.63827216,  9.34103516,  6.67778071,\n        9.16749109,  9.87942607,  8.09691001,  6.30103   ,  6.        ,\n        6.40450378,  4.99567863,  6.40926959,  6.48558508,  6.14605855,\n        6.59963473,  6.781202  ,  6.83003183,  6.70289635,  6.66015122,\n        7.59176003,  6.80327128,  7.63808338,  6.92081875,  4.50863831,\n        5.20065945,  4.87942607,  4.86327943,  5.69897   ,  7.09366496,\n        4.45593196,  4.55284197,  4.35654732,  4.20065945,  4.4436975 ,\n        4.16749109,  5.59516628,  7.10957898,  7.67778071,  7.67819452,\n        9.58335949,  6.92081875,  6.76955108,  9.57839607,  5.45593196,\n        6.        ])\n```\n:::\n:::\n\n\n::: {.cell execution_count=32}\n``` {.python .cell-code}\ntype(y_mp_test)\n```\n\n::: {.cell-output .cell-output-display execution_count=32}\n```\nnumpy.ndarray\n```\n:::\n:::\n\n\nMean squared error between y_true (max phase null compounds' pKi values) and y_pred\n\nMSE - closer to zero, the better the model is (less errors present)\n\n::: {.cell execution_count=33}\n``` {.python .cell-code}\nfrom sklearn.metrics import mean_squared_error\nmean_squared_error(y_true, y_mp_test)\n```\n\n::: {.cell-output .cell-output-display execution_count=33}\n```\n2.330350086602576\n```\n:::\n:::\n\n\nR2 - closer to 1, the better the model is (negative likely model was not performing as well as expected or as quoted from scikit-learn \"arbitrarily worse\")\n\n::: {.cell execution_count=34}\n``` {.python .cell-code}\nfrom sklearn.metrics import r2_score\nr2_score(y_true, y_mp_test)\n```\n\n::: {.cell-output .cell-output-display execution_count=34}\n```\n-0.12911187644281164\n```\n:::\n:::\n\n\nBecause the data was re-sampled in a iblr-gn way, the size of array would be different from the original dataset, so grabbing the iblr-gn modified data.\n\n::: {.cell execution_count=35}\n``` {.python .cell-code}\ny_true_gn = data_gn_mp_null[\"pKi\"]\ny_true_gn = y_true_gn.to_numpy(copy=True)\n```\n:::\n\n\n::: {.cell execution_count=36}\n``` {.python .cell-code}\n# R squared for iblr-gn data\n#type(y_true_gn)\nr2_score(y_true_gn, y_mp_gn_test)\n```\n\n::: {.cell-output .cell-output-display execution_count=36}\n```\n-0.9094246996499529\n```\n:::\n:::\n\n\n::: {.cell execution_count=37}\n``` {.python .cell-code}\n# MSE for iblr-gn data\nmean_squared_error(y_true_gn, y_mp_gn_test)\n```\n\n::: {.cell-output .cell-output-display execution_count=37}\n```\n6.26404309531457\n```\n:::\n:::\n\n\nWell, it appeared iblr-gn data might not offer much advantage to the original max phase split method, even the max phase split method wasn't that great either.\n\n<br>\n\n##### **Feature importances**\n\nThere were two types of feature importances available in *scikit-learn*, which I've described below.\n\n<br>\n\n###### **feature_importances_ attribute from *scikit-learn***\n\nThe impurity-based feature importances (also known as Gini importance).\n\n::: {.cell execution_count=38}\n``` {.python .cell-code}\n# Compute feature importances on rfreg training model\nfeature_imp = rfreg.feature_importances_\n```\n:::\n\n\n::: {.cell execution_count=39}\n``` {.python .cell-code}\n# Check what feature_imp looks like (an array)\nfeature_imp\n```\n\n::: {.cell-output .cell-output-display execution_count=39}\n```\narray([0.01845843, 0.0833086 , 0.00414526, 0.17344377, 0.02473391,\n       0.03119123, 0.03215709, 0.01706784, 0.        , 0.21772693,\n       0.03390372, 0.16184532, 0.07451324, 0.029626  , 0.00256679,\n       0.01350827, 0.02132237, 0.00666429, 0.02731864, 0.        ,\n       0.01323578, 0.01326253])\n```\n:::\n:::\n\n\n::: {.cell execution_count=40}\n``` {.python .cell-code}\n# Convert the feature_imp array into dataframe\nfeature_imp_df = pd.DataFrame(feature_imp)\n#feature_imp_df\n\n# Obtain feature names via column names of dataframe\n# Rename the index as \"features\"\nfeature = X_mp4_df.columns.rename(\"features\")\n\n# Convert the index to dataframe\nfeature_name_df = feature.to_frame(index = False)\n\n# Concatenate feature_imp_df & feature_name_df\nfeature_df = pd.concat([feature_imp_df, feature_name_df], axis=1)\n\n# Rename the column for feature importances\nfeature_df = feature_df.rename(columns = {0: \"feature_importances\"})\n\n# Sort values of feature importances in descending order\nfeature_df = feature_df.sort_values(\"feature_importances\", ascending=False)\n```\n:::\n\n\n::: {.cell execution_count=41}\n``` {.python .cell-code}\n# Seaborn bar plot\nsns.barplot(feature_df, x = \"feature_importances\", y = \"features\")\n```\n\n::: {.cell-output .cell-output-display execution_count=41}\n```\n<Axes: xlabel='feature_importances', ylabel='features'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](1_random_forest_files/figure-html/cell-42-output-2.png){width=732 height=429}\n:::\n:::\n\n\nAn alternative way to plot was via Matplotlib directly (note: Seaborn also uses Matplotlib as well, so the plots are pretty similar). The code below were probably a bit more straightforward but without axes named and values were not sorted.\n\n::: {.cell execution_count=42}\n``` {.python .cell-code}\n# Matplotlib plot\nfrom matplotlib import pyplot as plt\nplt.barh(X_mp4_df.columns, rfreg.feature_importances_)\n```\n\n::: {.cell-output .cell-output-display execution_count=42}\n```\n<BarContainer object of 22 artists>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](1_random_forest_files/figure-html/cell-43-output-2.png){width=714 height=411}\n:::\n:::\n\n\n<br>\n\n###### **permutation_importance function from *scikit-learn***\n\nThere were known issues with the built-in feature_importances_ attribute in *scikit-learn*. As quoted from *scikit-learn* on [feature importance evaluation](https://scikit-learn.org/stable/modules/ensemble.html#feature-importance-evaluation):\n\n>... The impurity-based feature importances computed on tree-based models suffer from two flaws that can lead to misleading conclusions. First they are computed on statistics derived from the training dataset and therefore do not necessarily inform us on which features are most important to make good predictions on held-out dataset. Secondly, they favor high cardinality features, that is features with many unique values. Permutation feature importance is an alternative to impurity-based feature importance that does not suffer from these flaws. ...\n\nSo here I've tried to use the model-agnostic permutation_importance function.\n\n::: {.cell execution_count=43}\n``` {.python .cell-code}\nfrom sklearn.inspection import permutation_importance\n\nperm_result = permutation_importance(rfreg, X_mp_test, y_mp_test, n_repeats=10, random_state=1, n_jobs=2)\n```\n:::\n\n\n::: {.cell execution_count=44}\n``` {.python .cell-code}\nsorted_perm_result = perm_result.importances_mean.argsort()\n\n# An array\nsorted_perm_result \n\n# Convert array into df\nperm_result_df = pd.DataFrame(sorted_perm_result)\nperm_result_df\n\n# Write a function to convert array to df leading to plots - for use in feature_importances_ & permutation_importance\n\n```\n\n::: {.cell-output .cell-output-display execution_count=44}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>11</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n<br>\n\n##### **Hyperparameter tuning**\n\n::: {.cell execution_count=45}\n``` {.python .cell-code}\n# Import additional ibraries\nfrom numpy import mean, std\n# RepeatedStratifiedKFold usually for binary or multi-class labels - ref link: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold\nfrom sklearn.model_selection import cross_val_score, RepeatedKFold\n```\n:::\n\n\n* Cross validations & hyperparameter tuning \n1. number of trees (n_estimators) \n\n::: {.cell execution_count=46}\n``` {.python .cell-code}\n# ---Evaluate a list of models with different number of trees---\n\n# Define dataset by using the same training dataset as above---\nX, y = X_mp4, y_mp4\n\n# Define function to generate a list of models with different no. of trees---\ndef models():\n    # Create empty dictionary (key, value pairs) for models\n    models = dict()\n    # Test different number of trees to evaluate\n    no_trees = [50, 100, 250, 500, 1000]\n    for n in no_trees:\n        models[str(n)] = RandomForestRegressor(n_estimators=n)\n    return models\n\n\n# Define function to evaluate a single model using cross-validation---\ndef evaluate(model, X, y):\n    # Define evaluation process\n    cross_val = RepeatedKFold(n_splits=10, n_repeats=15, random_state=1)\n    # Run evaluation process & collect cv scores\n    # Since estimator/model was based on DecisionTreeRegressor, using neg_mean_squared_error metric\n    # n_jobs = -1 meaning using all processors to run jobs in parallel\n    scores = cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=cross_val, n_jobs=-1)\n    return scores\n\n\n# Evaluate results---\n# Run models with different RepeatedKFold & different no. of tress\n# with results shown as diff. trees with calculated mean cv scores & std\n\n# Obtain diff. models with diff. trees via models function\nmodels = models()\n\n# Create empty lists for results & names\nresults, names = list(), list()\n\n# Create a for loop to iterate through the list of diff. models\nfor name, model in models.items():\n    # Run the cross validation scores via evaluate function\n    scores = evaluate(model, X, y)\n    # Collect results\n    results.append(scores)\n    # Collect names (different no. of trees)\n    names.append(name)\n    # Show the average mean squared errors and corresponding standard deviations \n    # for each model with diff. no. of trees\n    print((name, mean(scores), std(scores)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n('50', -1.648674793829449, 1.6658803361885683)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n('100', -1.6854413484624389, 1.6418670624539684)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n('250', -1.652859976152396, 1.6277707822684215)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n('500', -1.6536872099732232, 1.6094571583353725)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n('1000', -1.6439992782991453, 1.59991822419288)\n```\n:::\n:::\n\n\nBest model performance would be the one with the most negative value for average mean squared error (note: the random forest algorithm was stochastic in nature, so every time it was run, it would provide different results due to random bootstrap sampling, so there wouldn't be a fixed answer). The negated version of the same value was due to how the scoring parameter source code was written in scikit-learn, which was written this way to take into account of both *scoring* functions and *loss* functions (please see provided links below). When the number of trees went past 500 and reaching 1000, we could see an increase in the average mean squared error (the value being less negative), meaning the error increased.\n\n* Links to help understanding neg_mean_squared_error:\n\n1. scikit-learn source code - https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/metrics/_scorer.py#L624\n\n2. StackOverflow answer - https://stackoverflow.com/questions/48244219/is-sklearn-metrics-mean-squared-error-the-larger-the-better-negated\n\n::: {.cell execution_count=47}\n``` {.python .cell-code}\n# Matplotlib boxplots for each no. of tree model with average mean squared errors shown\nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](1_random_forest_files/figure-html/cell-48-output-1.png){width=569 height=411}\n:::\n:::\n\n\n::: {.cell execution_count=48}\n``` {.python .cell-code}\n# Try Seaborn version too\n```\n:::\n\n\n::: {.cell execution_count=49}\n``` {.python .cell-code}\n# Show all scoring metrics - URL link: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n# sklearn.metrics.get_scorer_names() \n```\n:::\n\n\n*Not coding for the ones below (due to length of post), but they should be looked into if doing full-scale, comprehensive ML models using random forest:\n\n- tree depths (max_depth)\n\n- number of samples (max_samples) (probably won't do this as the training sample size was already very small to start with!)\n\n- number of features (max_features) (can mention using RDKit's version to generate molecular features which would provide 209)\n\n- nodes\n\n* Might not cover SHAP value for feature importances (due to post length!), but mention this was also useful as it was a model-agnostic method that could be applied in many scenarios\n\n* Plots - Black-box ML e.g. if comparing clogp vs. pKi? (unlike white-box ML for decision tree) - or can mention that the feature importances section was necessary to shed some lights and remove some layers of the black-box style of random forest by showing which features were making impacts on the predictive models.\n\n* Other options available in Scikit-learn ensemble methods e.g. voting classifier/regressor or stacking models to reduce biases\n\n* Aim to keep post short and succinct!\n\n",
    "supporting": [
      "1_random_forest_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}